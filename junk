<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Curated Tech News</title><link>https://darren.insidemind.com.au/news.html</link><description>AI-curated tech news. These are links from around the web - none of this is my own work.</description><lastBuildDate>Sun, 25 Jan 2026 05:51:34 +0000</lastBuildDate><generator>news-feed curated by Darren Oakey</generator><item><title>Build Advanced RAG with LangGraph</title><link>https://pub.towardsai.net/build-advanced-rag-with-langgraph-329158669812?source=rss----98111c9905da---4</link><description>image by author Bringing together Self-reflection, Corrective, and Adaptive systems We all know and love Retrieval-Augmented Generation (RAG). The simplest implementation of Retrieval-Augmented Generation (RAG) is a vector store with documents connected to a Large Language Model to generate a response from a query. This is known as a Single Step Approach . For simple queries this strategy is quite effective, especially if the documents have the answer the user is looking for. However, LangGraph introduced architecture that can enhance the RAG system for advanced capabilities: Self-reflective Retrieval Augmented Generation is when the system checks and critiques its own answers and/or the retrieved evidence, and then uses that reflection to decide whether to retrieve more information, revise the answer, or stop. Inspired from the paper: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection Corrective Retrieval Augmented Generation is designed to detect when retrieved documents are wrong or insufficient and then correct the retrieval before answering. Inspired from the paper: Corrective Retrieval Augmented Generation Adaptive Retrieval Augmented Generation is an approach where the system dynamically decides whether , when , and how much to retrieve external information based on the question, instead of always retrieving from the vector store. inspired from the paper: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity In this article I will take you through the steps to combine all these features together into one powerhouse RAG system! üëâCheck out the Sources below for more information! üëâCheck out the Github repo for all the code! üö®In order to build this system you will need: - an API Key for OpenAI -TAVILY API Key ü§ñ Tavily is how we will do our web searches, its a great tool! Architecture image by author And let‚Äôs break it down: If Vector Store is selected: 1a. If Vector Store has relevant documents -&gt; Then generate an answer -&gt;&gt; If Answer is relevant to Question -&gt;&gt;&gt; If Hallucinations DO NOT exist -&gt;&gt;&gt;&gt; Return the response. 2a. If Vector Store DOES NOT have relevant documents -&gt; Then Web search -&gt;&gt; Then generate an answer -&gt;&gt; If Answer is relevant to Question -&gt;&gt;&gt; If Hallucinations DO NOT exist then return the response. -&gt;&gt;&gt;&gt; Return the response. 2. If Web Search is selected: -&gt;&gt; Then generate an answer -&gt;&gt; If Answer is relevant to Question -&gt;&gt;&gt; If Hallucinations DO NOT exist then return the response. -&gt;&gt;&gt;&gt; Return the response. 3. After response is Generated: 3a. If Answer is not relevant to the Question -&gt; Check for Hallucinations -&gt;&gt;If Hallucinations Exist Generate a new response -&gt;&gt;&gt; Check if Answer is relevant to the Question -&gt;&gt;&gt;&gt;Continue on with logic flow 3b. If Answer is not relevant to the Question -&gt; Check for Hallucinations -&gt;&gt;If Hallucinations DO NOT Exist go to Web Search -&gt;&gt;&gt;Continue on with logic flow Files we will be using: graph/ - chains/ -- answer_grader.py -- generation.py -- hallucination_grader.py -- retrieval_grader.py -- router.py - nodes/ -- generate.py -- grade_documents.py -- retrieve.py -- web_search.py - consts.py - state.py ingestion.py main.py graphs/chains and graph/nodes mapped out: image by author LangGraph LangGraph is a workflow that will create the logic flow for your agent behavior. It has three components: State : A shared data structure reflecting the application‚Äôs current state. Nodes : Functions that encode the logic for your agent and do a computation. The input is the current state and the output is an update to that state. Edges: Functions that decide which Node to fire based off the current state. They can be conditional branches or fixed transitions. Nodes are the workers and edges use the output of the node to make a logical determination of the next path foward. Ingestion First step is to create out vector database, for this I used Chroma. üëâChroma is a developer-friendly local vector database. It‚Äôs scalability is. limited, has persistent storage and has great integration with LangChain Chains Chains represent linear, sequential flows. Traditional chains are generally acyclic and pass data forward linearly. We have four: generation: Create the LLM that can take in prompts from a web search, returns a RESPONSE router: LLM evaluator on if the question should go to the vector store or a web search, returns YES or NO answer_grader: Create an LLM evaluator to assess if the response is relevant to the question, returns YES or NO hallucination_grader : Create an LLM evaluator to assess if the response is relevant to the question, returns YES or NO retrieval_grader : Create an LLM evaluator to assess if the retrieved documents is relevant to the question, returns YES or NO Nodes In LangGraph Nodes are functional, independent units of computation (e.g., an LLM call or tool execution) that act on a shared state. Nodes allow for cyclic graphs, complex branching, and stateful, agentic workflows We have four nodes: retrieve: If router has chosen RAG then bind the vector store with the question to retrieve the documents. generate : If Vector Store is chose by the router , then generate a response from RAG. Send question and documents the LLM for a response. grade_documents : Go through a list of retrieved documents, call the retrieval_grader and evaluate each document. If a document is not relevant then do a web search. web_search: If Web Search was chosen, then use Tavily to crawl the web. Select 3 webpages, concat the text into one string and send question and string to the LLM for a response. Edges Let‚Äôs do another review of our graph, if we recall we had three areas where a binary decision will be made. If we recall, edges are functions that decide which Node to fire based off the current state. We have three: - route_question: Takes the input question and sends it to router, returns binary response from router (to do websearch or not) - grade_generation_grounded_in_documents_and_question: Takes the retrieved documents and the generation, and sends it to the hallucination_grader, returns binary response if hallucinations are present -decide_to_generate : Takes the output from grade_documents and makes the the next code call, returns a binary response if the system can proceed with generation or do a web search. image by author Graph The graph in LangGraph simply brings all the pieces together. At this point you should have your nodes and edges defined, along with your state. First thing to do is add nodes to the graph: add_node Adds a new node to the StateGraph, need a label for the node, followed by the function name. RETRIEVE = "retrieve" GRADE_DOCUMENTS = "grade_documents" GENERATE = "generate" WEBSEARCH = "websearch" workflow = StateGraph(GraphState) workflow.add_node(RETRIEVE, retrieve) workflow.add_node(GRADE_DOCUMENTS, grade_documents) workflow.add_node(GENERATE, generate) workflow.add_node(WEBSEARCH, web_search) set_conditional_entry_point Decide which node the graph should start from at runtime, based on the initial input or state, instead of always starting from a fixed node. In our case it calls the route_question edge. The conditional mapping dictionary maps outer output to the graph node name. workflow.set_conditional_entry_point( route_question, { WEBSEARCH: WEBSEARCH, RETRIEVE: RETRIEVE, }, ) Now let‚Äôs define a fixed transition in a flow using add_edge. After the RETRIEVE node finishes, the workflow should automatically move to the GRADE_DOCUMENTS node. workflow.add_edge(RETRIEVE, GRADE_DOCUMENTS) Now, I think we are getting the hang of it? We just follow the flow in the graph above and keep going along: workflow.add_conditional_edges( GRADE_DOCUMENTS, decide_to_generate, { WEBSEARCH: WEBSEARCH, GENERATE: GENERATE, }, ) workflow.add_conditional_edges( GENERATE, grade_generation_grounded_in_documents_and_question, { "not supported": GENERATE, "useful": END, "not useful": WEBSEARCH, }, ) workflow.add_edge(WEBSEARCH, GENERATE) workflow.add_edge(GENERATE, END) app = workflow.compile() Test Vector Search Question : What is chain of thought in task Decomposition in an Agent System? ---ROUTE QUESTION--- ---ROUTE QUESTION TO RAG--- ---RETRIEVE--- ---CHECK DOCUMENT RELEVANCE TO QUESTION--- ---GRADE: DOCUMENT RELEVANT--- ---GRADE: DOCUMENT RELEVANT--- ---GRADE: DOCUMENT NOT RELEVANT--- ---GRADE: DOCUMENT RELEVANT--- ---ASSESS GRADED DOCUMENTS--- ---DECISION: NOT ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH--- ---WEB SEARCH--- ---GENERATE--- ---CHECK HALLUCINATIONS--- ---DECISION: GENERATION IS GROUNDED IN DOCUMENTS--- ---GRADE GENERATION vs QUESTION--- ---DECISION: GENERATION ADDRESSES QUESTION--- Response: "Chain of thought (CoT) in task decomposition involves breaking down complex tasks into clear, logical steps to enhance model performance. It transforms big tasks into manageable ones and provides insight into the model's thinking process. Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure for problem-solving." Web Search Question : What is blast used for in bioinformatics? ---ROUTE QUESTION--- ---ROUTE QUESTION TO WEB SEARCH--- ---WEB SEARCH--- ---GENERATE--- ---CHECK HALLUCINATIONS--- ---DECISION: GENERATION IS GROUNDED IN DOCUMENTS--- ---GRADE GENERATION vs QUESTION--- ---DECISION: GENERATION ADDRESSES QUESTION--- Response: "BLAST is used in bioinformatics to compare DNA, RNA, or protein sequences against a database to find similar sequences. It is essential in genome annotation, finding gene mutations linked to diseases, and comparing species in evolutionary biology. BLAST is a powerful tool for sequence analysis and is commonly used in clinical diagnostics." Okay, looking good so far! Let‚Äôs ask a question that will cause the system to hallucinate and see how it responds. Question : Who was the first CEO of Google Brain Labs Europe? üëâThis is a notorious question for assessing hallucinations in your architecture because Google Brain Labs Europe does not exist. ---ROUTE QUESTION--- ---ROUTE QUESTION TO WEB SEARCH--- ---WEB SEARCH--- ---GENERATE--- ---CHECK HALLUCINATIONS--- ---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY--- ---GENERATE--- ..... There are a ton of more testing we can do, but I think this exemplifies what the system is capable of! Wow, what a journey! I hope you liked this tutorial and I hope it inspires you to create your own advanced RAG systems! Sources: https://medium.com/media/99e88d73094dbc52bb3f631a89464cea/href Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection https://medium.com/media/869e1710c3e67736574965e832ef1473/href Self-Reflective RAG with LangGraph langgraph/examples/rag/langgraph_adaptive_rag_cohere.ipynb at main ¬∑ langchain-ai/langgraph https://github.com/emarco177/langgraph-course/tree/project/agentic-rag Build Advanced RAG with LangGraph was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story.</description><pubDate>Sun, 25 Jan 2026 03:17:37 GMT</pubDate><guid isPermaLink="false">https://medium.com/p/329158669812</guid><source url="https://pub.towardsai.net/feed">Towards AI</source><score>8.3</score></item><item><title>Solving The Add Two Numbers Leetcode Question</title><link>https://dev.to/ehizman/solving-the-add-two-numbers-leetcode-question-1a5o</link><description>Question You are given the starting nodes of two LinkedLists. Each LinkedList represents a reversed integer value: that is the value 768 is represent in linked list form as 8-&gt;6-&gt;7 . The task is to add the numbers in the linked list and then return the head of a new linked list representing the sum with the numbers reversed. Clarifying Questions Are the values of the ListNodes between 0 and 9? This question is important because we want to keep track of carry values after addition. What are the maximum values represented by a LinkedList? This question is important because we want to it would determined the approach to take. If the maximum values represented by a linkedlist can be the INTEGER.MAX_VALUE then this means that we cannot "integerify" both linked lists and then convert the result to a list and then return the head. Do both input lists have the same number of nodes? This question would determine the constraints in your solution. Approach 1: Brute Force The brute force approach to solving this is "integerify" both input lists, add the resulting integers and then finally convert the sum to a list. /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ class Solution { public ListNode addTwoNumbers ( ListNode l1 , ListNode l2 ) { int l1Str = convertToInt ( l1 ); int l2Str = convertToInt ( l2 ); int sum = l1Str + l2Str ; ListNode result = convertToList ( sum ); return result ; } private ListNode convertToList ( int num ){ ListNode head = new ListNode ( num % 10 ); num = num / 10 ; ListNode current = head ; while ( num &gt; 0 ){ current . next = new ListNode ( num % 10 ); num = num / 10 ; current = current . next ; } return head ; } private int convertToInt ( ListNode l ) { ListNode current = l ; StringBuilder sb = new StringBuilder (); while ( current != null ) { sb . append ( current . val ); current = current . next ; } return Integer . parseInt ( sb . reverse (). toString ()); } } The time complexity for this solution is O(max(N,M)) and space complexity is O(n) . However this solution fails for the edge case 9999999991 and 9 because 9999999991 overflows the java integer data type. Approach 2: My second approach was to traverse both lists and add the corresponding digits individually. This meant that I needed to track the carry (most significant digit after every addition), while I create a new list node with the least significant digit as the val . I created two ListNode variables one to keep track of the result and the other to keep track of the current additions. /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ class Solution { public ListNode addTwoNumbers ( ListNode l1 , ListNode l2 ) { int carry = 0 ; ListNode dummy = new ListNode ( 0 ); ListNode current = dummy ; while ( l1 != null || l2 != null || carry != 0 ) { int l1Val = l1 != null ? l1 . val : 0 ; int l2Val = l2 != null ? l2 . val : 0 ; int sum = l1Val + l2Val + carry ; current . next = new ListNode ( sum % 10 ); carry = sum / 10 ; current = current . next ; if ( l1 != null ){ l1 = l1 . next ; } if ( l2 != null ) { l2 = l2 . next ; } } return dummy . next ; } } Time Complexity: O(max(n,m)) Space Complexity: O(n)</description><pubDate>Sat, 24 Jan 2026 23:09:50 +0000</pubDate><guid isPermaLink="false">https://dev.to/ehizman/solving-the-add-two-numbers-leetcode-question-1a5o</guid><source url="https://dev.to/feed">Dev.to</source><score>9.0</score></item><item><title>The #1 sentence I add to prompts that makes them better</title><link>https://dev.to/jjn1056/the-1-sentence-i-add-to-prompts-that-makes-them-better-514n</link><description>Prompt Engineering or selling your soul? If you use AI at work, you‚Äôve probably had this experience: You write a prompt that feels clear enough , hit enter, and the model confidently produces something that is‚Ä¶ technically fine‚Ä¶ but not what you meant. Maybe it built the right thing in the wrong style. Maybe it chose an approach you would never ship. Maybe it made assumptions you didn‚Äôt realize you were leaving unstated. I‚Äôve started treating prompts like a contract with the devil. Not because I think AI is evil ‚Äî just because it‚Äôs literal , opportunistic, and perfectly willing to sprint in the wrong direction if you give it even a small opening. And you can‚Äôt cover every edge case up front. So here‚Äôs the one sentence I add to a lot of my prompts that consistently makes the results better: Before you begin, ask any clarifying questions you need to fully understand what I‚Äôm asking and to do an excellent job. Why it works (and why it‚Äôs ‚Äúvibe engineering‚Äù) Most prompting advice is basically: be more specific. That‚Äôs true, but it‚Äôs incomplete ‚Äî because the whole problem is that you often don‚Äôt realize what you forgot to specify . This sentence flips the dynamic: Instead of ‚ÄúI describe something and hope the AI guesses right,‚Äù it becomes collaborative. It turns the model into a reviewer before it becomes an implementer. And it forces the ‚Äúunknown unknowns‚Äù to show up early, while it‚Äôs still cheap to correct. The kind of questions that save you My favorite clarifying questions are the ones that expose missing context I didn‚Äôt realize mattered. Like: ‚ÄúIs there an existing system you want me to use as a template?‚Äù ‚ÄúIs this a large table / high-traffic database?‚Äù ‚ÄúIs this safe to run during business hours?‚Äù ‚ÄúWhat does success look like: correctness, speed, low risk, or minimal code changes?‚Äù ‚ÄúDo you care about test coverage, or just a working fix?‚Äù Those questions aren‚Äôt just helpful to the AI. They‚Äôre helpful to me. Because half the time, I‚Äôm using the AI to tease out details I forgot to include in the prompt in the first place. A realistic failure mode this prevents Say you ask: ‚ÄúWrite a migration to backfill X safely.‚Äù The AI might happily generate a perfectly valid migration that: locks the table, runs as one big transaction, does a full scan, adds an index in a way that takes forever, and generally assumes the world is a small quiet sandbox. It‚Äôs not ‚Äúwrong‚Äù ‚Äî it‚Äôs just not something you want to discover after you already committed to the approach. If you make the model interview you first, you‚Äôll often get the question you forgot to say out loud: ‚ÄúHow large is the table, and can this run on prod without blocking writes?‚Äù That one question can save you a very annoying afternoon. When I don‚Äôt use it I don‚Äôt paste this sentence into every single prompt. If I‚Äôm asking something small and obvious (‚Äúwrite this one-liner‚Äù, ‚Äúexplain this error‚Äù, ‚Äúrename these variables‚Äù), it‚Äôs unnecessary overhead. But if I‚Äôm doing any of these, it comes out almost automatically: writing something new (a real feature, not a snippet) debugging a complex problem anything with multiple moving parts anything where ‚Äútechnically correct‚Äù can still waste time Copy/paste template you can steal Here‚Äôs a version you can drop into your own prompts: My default prompt preamble Before you begin, ask any clarifying questions you need to fully understand what I‚Äôm asking and to do an excellent job. Optional follow-up (if you want to be extra explicit) If something is ambiguous, don‚Äôt guess ‚Äî ask. If there are multiple valid approaches, list the options and tell me what you recommend and why. After questions are answered, produce the output in a clean, usable format. That‚Äôs it. It‚Äôs not magic. It doesn‚Äôt make the model smarter. It just keeps the process collaborative instead of ‚Äúwrite a prompt and hope for the best.‚Äù Teaser: if you want to hear about the #2 sentence I add to prompts to make them even better, get me to 100 likes üòÑ</description><pubDate>Sun, 25 Jan 2026 00:16:51 +0000</pubDate><guid isPermaLink="false">https://dev.to/jjn1056/the-1-sentence-i-add-to-prompts-that-makes-them-better-514n</guid><source url="https://dev.to/feed">Dev.to</source><score>8.4</score></item><item><title>If your USB-C device isn't charging, here's the quick fix I always consider first</title><link>https://www.zdnet.com/article/why-your-usb-c-device-isnt-charging-and-my-fix/</link><description>Your phone, tablet, or other gadget won't charge when connected to a USB-C cable? Here's why, and how to fix it.</description><pubDate>Fri, 23 Jan 2026 16:59:00 GMT</pubDate><guid isPermaLink="false">62314f59-f794-4691-acb3-f884bddc48f2</guid><source url="https://www.zdnet.com/topic/big-data/rss.xml">ZDNet Big Data</source><score>8.5</score></item><item><title>How I effectively turned my regular tablet into an e-reader (and it works on iPad or Android)</title><link>https://www.zdnet.com/article/how-to-turn-ipad-android-tablet-into-e-reader/</link><description>There's no need to buy a dedicated device if you're an avid reader; you can carry your library right in your tablet.</description><pubDate>Fri, 23 Jan 2026 19:38:23 GMT</pubDate><guid isPermaLink="false">80cbd260-5cb1-4890-8b44-21f08dc951d6</guid><source url="https://www.zdnet.com/topic/big-data/rss.xml">ZDNet Big Data</source><score>9.9</score></item><item><title>AI Agents Are Poised to Hit a Mathematical Wall, Study Finds</title><link>https://gizmodo.com/ai-agents-are-poised-to-hit-a-mathematical-wall-study-finds-2000713493</link><description>LLMs have their limits.</description><pubDate>Fri, 23 Jan 2026 19:55:53 +0000</pubDate><guid isPermaLink="false">https://gizmodo.com/?p=2000713493</guid><source url="https://gizmodo.com/rss">Gizmodo</source><score>8.2</score></item><item><title>The best digital notebooks of 2026: I tested notebooks from almost every price point</title><link>https://www.zdnet.com/article/best-smart-notebook/</link><description>I tested the the best digital notebooks to see which ones actually make it easier to capture ideas without paper clutter.</description><pubDate>Fri, 23 Jan 2026 13:01:06 GMT</pubDate><guid isPermaLink="false">82c00706-8f0a-4f55-bf97-0f59adcd22fc</guid><source url="https://www.zdnet.com/topic/big-data/rss.xml">ZDNet Big Data</source><score>8.7</score></item><item><title>This charger made alkaline batteries completely unnecessary for me in 2026</title><link>https://www.zdnet.com/article/xtar-l8-box-charger-review/</link><description>The Xtar L8 Box can power eight batteries simultaneously, is compatible with both AA and AAA types, and charges via USB-C.</description><pubDate>Fri, 23 Jan 2026 16:00:47 GMT</pubDate><guid isPermaLink="false">a4ba8b60-30a0-4bfc-aaf0-23477e1a042b</guid><source url="https://www.zdnet.com/topic/big-data/rss.xml">ZDNet Big Data</source><score>9.3</score></item><item><title>The Math on AI Agents Doesn‚Äôt Add Up</title><link>https://www.wired.com/story/ai-agents-math-doesnt-add-up/</link><description>A research paper suggests AI agents are mathematically doomed to fail. The industry doesn‚Äôt agree.</description><pubDate>Fri, 23 Jan 2026 16:00:00 +0000</pubDate><guid isPermaLink="false">696fc66418b82b3267d015a4</guid><source url="https://www.wired.com/feed/category/business/latest/rss">WIRED Business</source><score>8.3</score></item><item><title>Autocomplete Is Not Intelligence</title><link>https://pub.towardsai.net/autocomplete-is-not-intelligence-cfc866275c33?source=rss----98111c9905da---4</link><description>Why Yann LeCun Thinks the LLM Hype Is a Detour ‚Äî and Why ‚ÄúWorld Models + Physical AI‚Äù Might Be the Real Road to AGI Continue reading on Towards AI ¬ª</description><pubDate>Fri, 23 Jan 2026 16:01:03 GMT</pubDate><guid isPermaLink="false">https://medium.com/p/cfc866275c33</guid><source url="https://pub.towardsai.net/feed">Towards AI</source><score>8.9</score></item><item><title>Apple plans to make Siri an AI chatbot (2 minute read)</title><link>https://techcrunch.com/2026/01/21/apple-plans-to-make-siri-an-ai-chatbot-report-says/?utm_source=tldrai</link><description>Apple plans to transform Siri into a chatbot, similar to ChatGPT, with the expected launch integrated into iOS 27. The revamped Siri, codenamed "Campos," will support both voice and text inputs, marking a strategic shift due to competitive pressure. Apple has chosen Google's Gemini as its AI partner after evaluating options like OpenAI and Anthropic.</description><pubDate>Thu, 22 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="false">https://techcrunch.com/2026/01/21/apple-plans-to-make-siri-an-ai-chatbot-report-says/?utm_source=tldrai</guid><source url="https://bullrich.dev/tldr-rss/ai.rss">TLDR AI</source><score>9.0</score></item><item><title>Multiplex Thinking for Reasoning Tasks (GitHub Repo)</title><link>https://github.com/GMLR-Penn/Multiplex-Thinking?utm_source=tldrai</link><description>This implementation introduces token-wise branch-and-merge reasoning to enable more expressive multi-path computation while keeping token representations compact.</description><pubDate>Thu, 22 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="false">https://github.com/GMLR-Penn/Multiplex-Thinking?utm_source=tldrai</guid><source url="https://bullrich.dev/tldr-rss/ai.rss">TLDR AI</source><score>9.0</score></item><item><title>What 1,150 senior tech and business leaders shared about AI and automation (Sponsor)</title><link>https://camunda.com/state-of-agentic-orchestration-and-automation/?utm_medium=paid_leadgen&amp;amp;utm_source=tldr&amp;amp;utm_campaign=Report.StateOfAgenticOrchestrationAndAutomation.26Q1.EN&amp;amp;utm_content=jan22_newsletter</link><description>AI is everywhere, but something is holding orgs back from scaling and governing them in prod. Over 1,000 senior tech and business leaders spoke to Camunda about their challenges. In this report, you'll see how teams are managing risks and improving orchestration to deliver reliable AI agents. Get the report</description><pubDate>Thu, 22 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="false">https://camunda.com/state-of-agentic-orchestration-and-automation/?utm_medium=paid_leadgen&amp;amp;utm_source=tldr&amp;amp;utm_campaign=Report.StateOfAgenticOrchestrationAndAutomation.26Q1.EN&amp;amp;utm_content=jan22_newsletter</guid><source url="https://bullrich.dev/tldr-rss/ai.rss">TLDR AI</source><score>9.1</score></item><item><title>Are these $50 headphones on Amazon legit? I tested them and couldn't believe what I heard</title><link>https://www.zdnet.com/article/haylou-s40-headphones-review/</link><description>Budget-friendly headphones are a dime a dozen, so what makes the Haylou S40s worth buying? Here's my verdict after a week of use.</description><pubDate>Fri, 23 Jan 2026 12:10:00 GMT</pubDate><guid isPermaLink="false">9fc4981a-da2a-4cdf-a379-a08452665472</guid><source url="https://www.zdnet.com/topic/big-data/rss.xml">ZDNet Big Data</source><score>9.1</score></item><item><title>NASA and IBM built an AI to predict solar flares before they hit Earth</title><link>https://www.newscientist.com/article/2492865-nasa-and-ibm-built-an-ai-to-predict-solar-flares-before-they-hit-earth/?utm_campaign=RSS%7CNSNS&amp;utm_source=NSNS&amp;utm_medium=RSS&amp;utm_content=technology</link><description>An AI model trained on years of data from NASA‚Äôs Solar Dynamics Observatory can predict the sun‚Äôs future appearance and potentially flag dangerous solar flares</description><pubDate>Wed, 20 Aug 2025 15:00:54 +0100</pubDate><guid isPermaLink="false">2492865-nasa-and-ibm-built-an-ai-to-predict-solar-flares-before-they-hit-earth|2492865</guid><source url="https://www.newscientist.com/subject/technology/feed/">New Scientist Tech</source><score>8.6</score></item><item><title>The best GPS running watches for 2026</title><link>https://www.engadget.com/wearables/best-gps-running-watch-141513957.html?src=rss</link><description>Having the right GPS watch on your wrist whether you‚Äôre going for your first ever run or your umpteenth run can make all the difference. The best GPS running watches not only keep track of how far you‚Äôve run, but they track pace and other real-time metrics, advanced training features to help you hit your goals and, of course, precise distance measurements. Some models even provide offline maps for navigation, sleep tracking, recovery insights, and smart features that ‚Äúregular‚Äù smartwatches do. For those who need extra durability and lasting battery life, higher-end sport watches ‚Äî like some of the best Garmin watches ‚Äî are built to handle intense workouts, harsh weather and long runs. If you're training for a marathon, triathlon or just want a multisport option that can keep up with your lifestyle, these watches have the tech to support you. With so many options available, from entry-level models to the best running watches packed with advanced running metrics, it can be tricky to find the right fit. That‚Äôs why we‚Äôve rounded up our top picks to help you choose the perfect GPS watch for your training needs. Best GPS running watches for 2026 Other GPS running watches we tested Polar Pacer Pro The Polar Pacer Pro looked and felt quite similar to our top pick, and it mapped my outdoor runs accurately. However, Polar‚Äôs companion app is leagues behind Garmin‚Äôs with a confusing interface and a design that feels very much stuck in the past. It‚Äôs also $100 more expensive than our top pick. Amazfit Cheetah Pro The Amazfit Cheetah Pro tracked my outdoor runs accurately and Zepp‚Äôs companion app has a coaching feature much like Garmin‚Äôs adaptive training plans that can outline a routine for you to complete in preparation for a race or to achieve a specific goal. My biggest issue with it was that its touchscreen wasn‚Äôt very responsive ‚Äî it took multiple hard taps on the display to wake it, and often the raise-to-wake feature didn‚Äôt work, leaving me staring at a dark screen. What to consider before buying a GPS running watch GPS speed and accuracy The most important thing for a GPS running watch to have is fast, accurate GPS tracking. That might seem obvious, but it‚Äôs quite easy to get distracted by all of the other smart features most of these devices have. Since most of them can be worn all day long as standard sport watches, there‚Äôs a lot of (possibly unnecessary) fluff that looks good on paper but won‚Äôt mean much if the core purpose if the device is left unfulfilled. To that end, I paid particular attention to how long it took each device‚Äôs built-in GPS tracking to grab my location before a run, if it ever lost my spot and the accuracy of the generated maps. Also, the device should be smart enough to let you start tracking a run while the GPS looks for your location. Workout profiles and trackable metrics You may not be able to suss out GPS accuracy just by looking at a spec sheet (that‚Äôs where this guide can help), but you can check for features like supported workout profiles. That‚Äôs something you‚Äôll want to look into, even if your one and only activity is running. Check to make sure the best running watches you‚Äôre considering support all the kinds of running activities you like to do (outdoor runs, treadmill runs, etc) and any other workouts you may want to track with it. Most fitness wearables today aren‚Äôt one-trick ponies; you‚Äôll find a healthy number of trackable exercise modes on any sport watch worth its salt. That said, the number of workout profiles can be directly proportional to a device‚Äôs price: the higher-end the product, chances are the more specific, precise workouts it can monitor. In a similar vein, you‚Äôll want to check the trackable metrics of any watch you‚Äôre considering before you buy. Since we‚Äôre talking about the best GPS running watches, most will be able to track the basics like distance, heart rate and pace, and those are bare minimums. Some watches can monitor additional stats like speed, cadence, stride length, advanced running dynamics, aerobic and anaerobic training effect, intensity minutes and more. If you‚Äôre already a serious runner who trains for multiple races each year, or if you're a trail runner who needs elevation and navigation features, you‚Äôll want to dig into the spec sheet of the watch you‚Äôre considering to make sure it can track all of your most necessary metrics. Size and weight It‚Äôs worth checking out a watch‚Äôs case size and weight before going all-in on one. GPS running watches, and standard smartwatches as well, can have a few different sizes to choose from so you‚Äôll want to make sure you‚Äôre getting the best fit for your wrist. I have a smaller wrist, so I tend to avoid extra-large cases (anything over 42mm or so), especially if I intend on wearing the device all day long as my main timepiece. Weight, on the other hand, is a little less controllable, but typically smaller case sizes will save you a few grams in overall weight. For those who need durability, particularly trail runners or those tackling extreme conditions, devices like Garmin watches offer rugged builds that can handle rough terrain, impact, and extreme weather. Battery life Unlike regular smartwatches, GPS running watches have two types of battery life you‚Äôll need to consider: with GPS turned on and in ‚Äúsmartwatch‚Äù mode. The former is more important than the latter because most GPS running watches have stellar battery life when used just as a smart timepiece. You can expect to get multiple days on a single charge, with some surviving more than two weeks (with all day and night wear) before they need a recharge. Battery life with GPS turned on will be much shorter by comparison, but any GPS running watch worth its salt should give you at least 10-15 hours of life with the GPS being used continuously. The more you‚Äôre willing to spend, the higher that number typically gets, with some GPS running watches lasting for 40 hours while tracking your location. This article originally appeared on Engadget at https://www.engadget.com/wearables/best-gps-running-watch-141513957.html?src=rss</description><pubDate>Fri, 23 Jan 2026 10:00:37 +0000</pubDate><guid isPermaLink="false">5daa500c-3147-3dc9-8f5e-d508fa140873</guid><source url="https://www.engadget.com/rss.xml">Engadget</source><score>9.9</score></item><item><title>Volvo's new EX60 electric SUV promises to 'end range anxiety' &amp;mdash; and is built with Nvidia and Google tech</title><link>https://www.businessinsider.com/volvo-ex60-ev-electric-suv-specs-range-photos-2026-1</link><description>The Volvo XC60 is the company's best-selling car of all time. Its new sibling, the EX60, goes electric with 400 miles of range and Google's Gemini.</description><pubDate>2026-01-23T10:05:02Z</pubDate><guid isPermaLink="false">https://www.businessinsider.com/volvo-ex60-ev-electric-suv-specs-range-photos-2026-1</guid><source url="https://feeds.businessinsider.com/custom/all">Business Insider</source><score>8.2</score></item><item><title>AI could be an entry-level job killer &amp;mdash; or Gen Z's ticket to advancement</title><link>https://www.businessinsider.com/ai-gen-z-job-killer-or-opportunity-2026-1</link><description>Many young professionals are uneasy about AI automation, but they're leaning into it anyway, new research suggests.</description><pubDate>2026-01-23T10:53:01Z</pubDate><guid isPermaLink="false">https://www.businessinsider.com/ai-gen-z-job-killer-or-opportunity-2026-1</guid><source url="https://feeds.businessinsider.com/custom/all">Business Insider</source><score>8.9</score></item><item><title>I was an active runner for 10 years, then I burned out. Switching to walking helped me lose fat and feel energized again.</title><link>https://www.businessinsider.com/swapping-walking-running-helped-me-lose-fat-build-muscle-2026-1</link><description>I got sick of running and changed my routine to focus more on walking and strength-training. I lost more fat and felt calmer.</description><pubDate>2026-01-23T10:57:01Z</pubDate><guid isPermaLink="false">https://www.businessinsider.com/swapping-walking-running-helped-me-lose-fat-build-muscle-2026-1</guid><source url="https://feeds.businessinsider.com/custom/all">Business Insider</source><score>9.6</score></item><item><title>Why The AI Foundry by Tredence in Chennai is a Working Room for Builders, Not Another Conference</title><link>https://analyticsindiamag.com/ai-highlights/why-the-ai-foundry-by-tredence-in-chennai-is-a-working-room-for-builders-not-another-conference/</link><description>Participants will design and prototype production-ready AI systems in a closed-door workshop for senior practitioners. The post Why The AI Foundry by Tredence in Chennai is a Working Room for Builders, Not Another Conference appeared first on Analytics India Magazine .</description><pubDate>Fri, 23 Jan 2026 11:34:19 +0000</pubDate><guid isPermaLink="false">https://analyticsindiamag.com/?p=10184743</guid><source url="https://analyticsindiamag.com/feed/">Analytics India Magazine</source><score>8.3</score></item><item><title>World API by World Labs</title><link>https://www.producthunt.com/products/marble-by-world-labs</link><description>Programmable 3D worlds powered by Marble Discussion | Link</description><pubDate>2026-01-22T04:28:08-08:00</pubDate><guid isPermaLink="false">tag:www.producthunt.com,2005:Post/1066612</guid><source url="https://www.producthunt.com/feed">Product Hunt</source><score>8.4</score></item><item><title>Tonkotsu</title><link>https://www.producthunt.com/products/tonkotsu</link><description>Manage a team of coding agents from a doc Discussion | Link</description><pubDate>2026-01-09T09:29:57-08:00</pubDate><guid isPermaLink="false">tag:www.producthunt.com,2005:Post/1060714</guid><source url="https://www.producthunt.com/feed">Product Hunt</source><score>8.9</score></item><item><title>Forge Agent</title><link>https://www.producthunt.com/products/rightnow-ai</link><description>Swarm Agents That Turn Slow PyTorch Into Fast GPU Kernels Discussion | Link</description><pubDate>2026-01-22T16:24:44-08:00</pubDate><guid isPermaLink="false">tag:www.producthunt.com,2005:Post/1066855</guid><source url="https://www.producthunt.com/feed">Product Hunt</source><score>8.4</score></item><item><title>From Scattered SOPs to Smart AI Assistant: Building an Internal Knowledge Base with RAG</title><link>https://pub.towardsai.net/from-scattered-sops-to-smart-ai-assistant-building-an-internal-knowledge-base-with-rag-b31a43ce7ec2?source=rss----98111c9905da---4</link><description>It‚Äôs 3 AM. An employee urgently needs to know the company‚Äôs travel reimbursement policy before booking a flight. They dig through shared drives, old emails, and PDF folders for 30 minutes. Sound familiar? This problem cost us hundreds of hours annually until we built DocVault AI. this image is generated Via AI (Gemini) The Problem Every Company Faces If you work in any organization with more than 20 people, you know this pain: Employees spend 20% of their work time just searching for information SOPs are buried across shared drives, emails, and countless PDF folders Outdated documents cause compliance issues and confusion New hires get overwhelmed trying to learn where everything is Support teams answer the same policy questions over and over We‚Äôve all been there. You know the information exists somewhere. You just can‚Äôt find it. Or worse you find five versions of the same document and have no idea which is current. Meet DocVault AI: Your Internal Knowledge Assistant DocVault AI transforms your company documents into an intelligent assistant. Upload your SOPs, policies, training manuals, and process documents then employees can ask questions in plain English and get instant, accurate answers with source citations. No more hunting through folders. No more ‚ÄúDoes anyone know where the travel policy is?‚Äù in Slack. Just ask: ‚ÄúWhat‚Äôs our hotel reimbursement limit?‚Äù and get an immediate answer with the exact section of the policy. What Makes DocVault AI Different 1. Semantic Search That Actually Works Unlike basic keyword search, DocVault AI understands meaning. Ask ‚ÄúHow do I take time off?‚Äù and it finds your PTO policy even if the document says ‚Äúvacation request procedure.‚Äù 2. Zero Training Required It‚Äôs just a chat interface. If your employees can use ChatGPT, they can use DocVault AI. No manuals needed. 3. Self-Hosted Privacy Your sensitive internal docs stay on your infrastructure. No third-party SaaS. Complete control. 4. Open Source Freedom MIT licensed. Free to use. Customize it however you want. No vendor lock-in. Note: This project uses various third-party libraries and dependencies, each with their own licenses. Please review the individual package licenses before use in production. How It Works: The Technical Magic Under the hood, DocVault AI uses RAG (Retrieval-Augmented Generation) architecture: The Process: Documents are split into overlapping chunks (500‚Äì1500 characters) Each chunk gets a vector embedding think of it as a semantic fingerprint FAISS indexes these vectors for lightning-fast searches (milliseconds) When users ask questions, we find the most relevant chunks OpenAI generates accurate answers from those specific chunks The Stack: Backend: Node.js + Express Database: SQLite with Prisma ORM Vector Search: FAISS (runs on CPU, no GPU needed!) Embeddings: HuggingFace Transformers AI: OpenAI API (or any compatible service) Frontend: React 19 + Tailwind CSS Auth: JWT with role-based access The beauty? It runs entirely on CPU. No expensive GPU infrastructure required. Real Results: Before and After Before DocVault AI: Average time to find policy information: 15‚Äì20 minutes Support tickets about policies: 50+ per week Employee frustration: High After DocVault AI: Average query response time: &lt; 5 seconds Self-service resolution rate: 85% Employee satisfaction: Way up Real-World Use Cases HR Department ‚ÄúWhat‚Äôs our remote work policy?‚Äù ‚ÄúHow many sick days do I have?‚Äù ‚ÄúWhat‚Äôs the parental leave process?‚Äù IT Operations ‚ÄúHow do I reset a user‚Äôs VPN access?‚Äù ‚ÄúWhat‚Äôs the server backup procedure?‚Äù ‚ÄúHow do I provision a new laptop?‚Äù Compliance &amp; Legal ‚ÄúWhat are our data retention requirements?‚Äù ‚ÄúWhat‚Äôs the vendor approval process?‚Äù ‚ÄúWho needs to sign off on contracts over $10K?‚Äù Sales Team ‚ÄúWhat‚Äôs our discount approval process?‚Äù ‚ÄúHow do I handle enterprise deals?‚Äù ‚ÄúWhat‚Äôs included in our premium tier?‚Äù New Employee Onboarding ‚ÄúWhat benefits do I get after 3 months?‚Äù ‚ÄúHow do I submit expenses?‚Äù ‚ÄúWho do I contact for IT issues?‚Äù Why We Open-Sourced It Every company faces this problem. Instead of everyone building their own solution (or paying expensive SaaS fees), we created something the community can improve together. Plus, for sensitive internal documents, you need to self-host. Your SOPs, HR policies, and trade secrets shouldn‚Äôt live on someone else‚Äôs servers. DocVault AI gives you complete control. Getting Started in 5 Minutes git clone https://github.com/faddyk2/docvault-ai.git cd docvault-ai npm run setup cd backend &amp;&amp; cp .env.example .env Edit .env and add your OpenAI API key: OPENAI_API_KEY=sk-your-key-here JWT_SECRET=any-random-string DATABASE_URL=file:./dev.db Start everything: npm start Visit http://localhost:3000 and you‚Äôre ready to go. Default login: Email: admin@example.com Password: admin123 Upload your first document and start asking questions! Architecture: Simple but Powerful Document Processing Pipeline: Upload ‚Üí File validated Text extracted (PDF/DOCX/TXT/HTML) Split into overlapping chunks Generate embeddings Store in FAISS vector database Query Pipeline: User asks question Question converted to vector FAISS finds similar chunks (semantic search) Top 5 chunks sent to OpenAI AI generates answer with context Return answer + source citations What‚Äôs Coming Next We‚Äôre actively working on: Multi-language support for global teams PDF highlighting to show exact answer locations Slack/Teams integration for in-app queries Conversation memory for follow-up questions Analytics dashboard showing most searched topics Custom model support for fine-tuning on your terminology Document versioning to track policy changes Approval workflows for document updates Lessons Learned Building This 1. RAG Beats Fine-Tuning for Internal Docs Fine-tuning is expensive and hard to update. RAG lets you update your knowledge base instantly by just uploading new documents. 2. Chunk Overlap Matters We settled on 200-character overlap between chunks. This ensures answers don‚Äôt get cut off mid-sentence. 3. Source Citations Are Critical Employees don‚Äôt trust answers without sources. Showing the exact document reference builds confidence. 4. Semantic Search &gt; Keyword Search Users phrase things differently than your documents. ‚ÄúPTO‚Äù vs ‚Äúvacation‚Äù vs ‚Äútime off‚Äù semantic search handles all of them. 5. CPU-Based Embeddings Work Fine HuggingFace models run great on CPU. You don‚Äôt need expensive GPU infrastructure for this. 6. Role-Based Access Is Essential Admins manage documents. Regular users just query. Clean separation prevents chaos. Technical Challenges We Solved Challenge 1: FAISS Performance FAISS can be memory-intensive with large document sets. We optimized by: Using IVF (Inverted File) indexes for datasets &gt;10K documents Incremental updates instead of full rebuilds Lazy loading embeddings Challenge 2: Context Window Limits OpenAI models have token limits. We: Send only the top 5 most relevant chunks Truncate chunks smartly at sentence boundaries Prioritize by relevance score Challenge 3: Hallucination Prevention AI models can make things up. We: Always include source context Use strict system prompts Show source citations for verification Fallback to direct chunk text if AI fails When NOT to Use DocVault AI Be honest it‚Äôs not for everyone: Very small teams ( &lt; 5 people): Probably overkill No documentation: If you don‚Äôt have docs, this won‚Äôt help Real-time data: This is for static documents, not live databases No technical resources: You need someone who can run npm commands Security Considerations What we built in: JWT authentication Role-based access control Password hashing with bcrypt SQL injection protection (Prisma) CORS configuration Environment variable security What you should add: HTTPS in production Rate limiting Input sanitization Audit logging Backup procedures SSO integration (if needed) Community &amp; Contributions DocVault AI is growing: ‚≠ê Star the repo if you find it useful üêõ Found a bug? Open an issue on GitHub üí° Have feature ideas? PRs are welcome üìß Questions? Open a discussion We‚Äôre building this together. Your contributions make it better for everyone. Try It Today If your team wastes hours searching through documents every week, DocVault AI might just save you hundreds of hours per year. Links: GitHub: https://github.com/faddyk2/docvault-ai Stop searching. Start asking. From Scattered SOPs to Smart AI Assistant: Building an Internal Knowledge Base with RAG was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story.</description><pubDate>Fri, 23 Jan 2026 04:57:00 GMT</pubDate><guid isPermaLink="false">https://medium.com/p/b31a43ce7ec2</guid><source url="https://pub.towardsai.net/feed">Towards AI</source><score>8.2</score></item><item><title>A Match Made in Heaven? AI-driven Matching of Vulnerabilities and Security Unit Tests</title><link>https://arxiv.org/abs/2502.03365</link><description>arXiv:2502.03365v4 Announce Type: replace-cross Abstract: Software vulnerabilities are often detected via taint analysis, penetration testing, or fuzzing. They are also found via unit tests that exercise security-sensitive behavior with specific inputs, called vulnerability-witnessing tests. Generative AI models could help developers in writing them, but they require many examples to learn from, which are currently scarce. This paper introduces VuTeCo, an AI-driven framework for collecting examples of vulnerability-witnessing tests from Java repositories. VuTeCo carries out two tasks: (1) The "Finding" task to determine whether a unit test case is security-related, and (2) the "Matching" task to relate a test case to the vulnerability it witnesses. VuTeCo addresses the Finding task with UniXcoder, achieving an F0.5 score of 0.73 and a precision of 0.83 on a test set of unit tests from Vul4J. The Matching task is addressed using DeepSeek Coder, achieving an F0.5 score of 0.65 and a precision of 0.75 on a test set of pairs of unit tests and vulnerabilities from Vul4J. VuTeCo has been used in the wild on 427 Java projects and 1,238 vulnerabilities, obtaining 224 test cases confirmed to be security-related and 35 tests correctly matched to 29 vulnerabilities. The validated tests were collected in a new dataset called Test4Vul. VuTeCo lays the foundation for large-scale retrieval of vulnerability-witnessing tests, enabling future AI models to better understand and generate security unit tests.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2502.03365v4</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.4</score></item><item><title>Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification</title><link>https://arxiv.org/abs/2601.16098</link><description>arXiv:2601.16098v1 Announce Type: cross Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.16098v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.7</score></item><item><title>FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design</title><link>https://arxiv.org/abs/2601.15710</link><description>arXiv:2601.15710v1 Announce Type: cross Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15710v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.2</score></item><item><title>AgentSM: Semantic Memory for Agentic Text-to-SQL</title><link>https://arxiv.org/abs/2601.15709</link><description>arXiv:2601.15709v1 Announce Type: cross Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15709v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.7</score></item><item><title>DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking</title><link>https://arxiv.org/abs/2601.15518</link><description>arXiv:2601.15518v1 Announce Type: cross Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15518v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.4</score></item><item><title>Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference</title><link>https://arxiv.org/abs/2601.15333</link><description>arXiv:2601.15333v1 Announce Type: new Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15333v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.1</score></item><item><title>VideoPro: Adaptive Program Reasoning for Long Video Understanding</title><link>https://arxiv.org/abs/2509.17743</link><description>arXiv:2509.17743v3 Announce Type: replace Abstract: Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models' ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2509.17743v3</guid><source url="https://arxiv.org/rss/cs.CV">arXiv Computer Vision</source><score>8.4</score></item><item><title>Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion</title><link>https://arxiv.org/abs/2601.15829</link><description>arXiv:2601.15829v1 Announce Type: new Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15829v1</guid><source url="https://arxiv.org/rss/cs.CV">arXiv Computer Vision</source><score>9.3</score></item><item><title>Monadic Context Engineering</title><link>https://arxiv.org/abs/2512.22431</link><description>arXiv:2512.22431v5 Announce Type: replace-cross Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2512.22431v5</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>9.1</score></item><item><title>R-KV: Redundancy-aware KV Cache Compression for Reasoning Models</title><link>https://arxiv.org/abs/2505.24133</link><description>arXiv:2505.24133v4 Announce Type: replace Abstract: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2505.24133v4</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.4</score></item><item><title>MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators</title><link>https://arxiv.org/abs/2505.22777</link><description>arXiv:2505.22777v5 Announce Type: replace Abstract: Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, commonsense, or relevance.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2505.22777v5</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.6</score></item><item><title>SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning</title><link>https://arxiv.org/abs/2502.13753</link><description>arXiv:2502.13753v2 Announce Type: replace Abstract: Long-context understanding has emerged as a critical capability for large language models (LLMs). However, evaluating this ability remains challenging. We present SCALAR, a benchmark designed to assess citation-grounded long-context reasoning in academic writing. SCALAR leverages academic papers and their citation structure to automatically generate high-quality ground-truth labels without human annotation. It features controllable difficulty levels and a dynamic updating mechanism that mitigates data contamination. The benchmark includes two tasks: a multiple-choice QA format and a cloze-style citation prediction. We evaluate a range of state-of-the-art LLMs and find that the multiple-choice task effectively distinguishes model capabilities. While human experts achieve over 90% accuracy, most models struggle. The cloze-style task is even more challenging, with no model exceeding 50% accuracy. SCALAR provides a domain-grounded, continuously updating framework for tracking progress in citation-based long-context understanding.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2502.13753v2</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.7</score></item><item><title>NP-Hard Lower Bound Complexity for Semantic Self-Verification</title><link>https://arxiv.org/abs/2501.15446</link><description>arXiv:2501.15446v2 Announce Type: replace Abstract: We model Semantic Self-Verification (SSV) as the problem of determining whether a statement accurately characterizes its own semantic properties within a given interpretive framework that formalizes a challenge in AI safety and fairness: can an AI system verify that it has correctly interpreted rules intended to govern its behavior? We prove that SSV, in this specification, is NP-complete by constructing a polynomial-time reduction from 3-Satisfiability (3-SAT). Our reduction maps a 3-SAT formula to an instance of SSV involving ambiguous terms with binary interpretations and semantic constraints derived from logical clauses. This establishes that even simplified forms of semantic self-verification should face computational barriers. The NP-complete lower bound has implications for AI safety and fairness approaches that rely on semantic interpretation of instructions, including but not limited to constitutional AI, alignment via natural language, and instruction-following systems. Approaches where an AI system verify its understanding of directives may face this computational barrier. We argue that more realistic verification scenarios likely face even greater complexity.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2501.15446v2</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.4</score></item><item><title>LLM Prompt Evaluation for Educational Applications</title><link>https://arxiv.org/abs/2601.16134</link><description>arXiv:2601.16134v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.16134v1</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.4</score></item><item><title>Evaluating and Achieving Controllable Code Completion in Code LLM</title><link>https://arxiv.org/abs/2601.15879</link><description>arXiv:2601.15879v1 Announce Type: cross Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15879v1</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.9</score></item><item><title>LLM-in-Sandbox Elicits General Agentic Intelligence</title><link>https://arxiv.org/abs/2601.16206</link><description>arXiv:2601.16206v1 Announce Type: new Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.16206v1</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>9.3</score></item><item><title>No Reliable Evidence of Self-Reported Sentience in Small Large Language Models</title><link>https://arxiv.org/abs/2601.15334</link><description>arXiv:2601.15334v1 Announce Type: new Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.</description><pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15334v1</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.4</score></item><item><title>WhaleFlux Signals Shift to Architecting Enterprise AI in 2026</title><link>https://ai-techpark.com/whaleflux-signals-shift-to-architecting-enterprise-ai-in-2026/</link><description>As enterprise AI adoption moves beyond experimentation and into production, the industry is entering a new phase where system reliability, governance, and long-term operability matter more than model performance alone. WhaleFlux today announced its positioning as an AI system builder, reflecting a broader shift in how organizations deploy AI at scale. Over... The post WhaleFlux Signals Shift to Architecting Enterprise AI in 2026 first appeared on AI-Tech Park .</description><pubDate>Fri, 23 Jan 2026 08:15:00 +0000</pubDate><guid isPermaLink="false">https://ai-techpark.com/?p=233916</guid><source url="https://ai-techpark.com/category/ai/feed/">AI TechPark</source><score>8.5</score></item><item><title>Complexity Can't Be Eliminated. It Can Only Be Moved</title><link>https://dev.to/thesystemistsimon/complexity-cant-be-eliminated-it-can-only-be-moved-2am7</link><description>Cover Image Photo by Sunder Muthukumaran on Unsplash Six patterns that determine where complexity lives in your codebase ‚Äî and how to choose consciously A senior engineer made our API faster by caching responses. Query time dropped 80%. We celebrated. Two months later, the cache was stale. Data was wrong. Users complained. We spent weeks debugging cache invalidation. The speed didn't come from nowhere. The complexity didn't disappear. We just moved it. This pattern behaves like a conservation law from physics. Not perfectly, but close enough to be useful. Why Complexity Relocates (Not Disappears) In physics, certain quantities can't be created or destroyed. Only transformed or moved. Energy conservation says energy can't be created or destroyed, only converted (chemical to kinetic, kinetic to heat). Momentum conservation says total momentum stays constant in a closed system. Mass conservation says mass doesn't appear or disappear, just rearranges. These aren't guidelines. They're laws. You can't violate them. You can only work within them. Software has something similar: essential complexity (the inherent difficulty your problem requires) can only move, not disappear. Larry Tesler famously called it "Conservation of Complexity": complexity can't be eliminated, only moved. UX designers know Tesler's Law intimately. But while this principle is well-recognized in design circles, software architects rarely discuss it explicitly or apply it systematically. I've noticed we treat "simplification" as if we're eliminating complexity rather than relocating it. We don't measure both sides of the trade. We don't name what's actually being relocated. This isn't quite like physics conservation laws, where total energy stays exactly constant. Software complexity can increase or decrease. But there's a pattern, and a floor. Every problem has essential complexity , what Fred Brooks called the inherent difficulty of what you're trying to solve. Authentication must verify identity. Distributed systems must coordinate. These requirements create complexity that can only relocate, or be eliminated by dropping features entirely. You can't design it away. Then there's accidental complexity , from how we implement solutions. Poor abstractions, unnecessary indirection, tech debt. This can be eliminated through better design. When net complexity increases (code drops 40%, config grows 60%, net +20%), you're seeing accidental complexity added during relocation. When complexity genuinely disappears (deleting 500 lines of dead code), you're removing accidental complexity that never contributed to solving the problem. The pattern: essential complexity moves. Accidental complexity varies. And there's a floor: you can't simplify below essential complexity without losing functionality. To be precise: when we say "complexity relocates," we mean essential complexity (the irreducible difficulty of your problem domain). You can't simplify a tax calculation system below the complexity of the tax code itself. You can only choose where that essential complexity lives in your architecture. This explains why some systems resist simplification. You're not fighting bad design. You're hitting essential complexity. The question shifts: Where should this essential complexity live to minimize total cost? When you "simplify" a system, you're not eliminating complexity. You're relocating it. When you make a decision configurable instead of hardcoded, you haven't reduced the number of decisions. You've moved where the decision happens. When you cache data, you haven't eliminated the work of keeping data fresh. You've transformed query complexity into cache invalidation complexity. Understanding relocation patterns changes how you think about software design. You stop asking "how do I eliminate this complexity?" and start asking "where do I want this complexity to live?" Six patterns emerge consistently. We'll call them relocation patterns that behave like conservation laws. Not physics-perfect, but strong enough to guide architectural decisions. The Six Patterns Pattern 1: Complexity Relocation The caching story is a perfect example. Before caching, we had high query complexity: every request hit the database, queries were slow, load was high. Cache management complexity was zero because we didn't have a cache. After caching, query complexity dropped dramatically. Requests were fast, database load was low. But cache management complexity exploded. We now had staleness issues, invalidation logic, consistency problems, memory pressure. Total complexity didn't decrease. We moved it from "slow queries" to "cache management." The system felt simpler in one dimension and more complex in another. The essential complexity of data consistency didn't disappear. It moved from query time to cache invalidation. But if your cache implementation is inefficient, you've added accidental complexity on top. I've learned you can't eliminate complexity. You can only move it. The question isn't "how do I make this simpler?" The question is "where should this complexity live?" Consider adding an abstraction layer. Before abstraction, you have high duplication complexity: the same database query logic appears in twenty places. But you have low abstraction complexity because there's no layer to understand. After creating an ORM, duplication complexity drops to near zero. Database logic lives in one place. But abstraction complexity rises. Now you need to understand the ORM, its query builder, its caching behavior, its transaction handling. You didn't reduce total complexity. You traded duplication complexity for abstraction complexity. The essential complexity of database operations remains. You just centralized where it lives. Whether abstraction adds accidental complexity depends on design quality. Whether that's a good trade? Depends on your context. For a system with many developers, centralizing complexity in an abstraction that a few people deeply understand might be better than distributing complexity across the codebase where everyone encounters it. For a tiny system with two developers, the abstraction might not be worth it: the duplication is manageable, the abstraction is overhead. This is why "simplification" is such a loaded term. When someone says "let's simplify this," what they usually mean is "let's move complexity from where it bothers me to somewhere else." (Which, to be fair, is sometimes exactly what you want.) But recognize you're relocating complexity, not eliminating it. Where can complexity go? You can push it to infrastructure: move complexity from application code to Kubernetes, but now you need to understand Kubernetes. You can push it to configuration: move complexity from code to config files, but now configuration management becomes complex. You can push it to runtime: use dynamic dispatch instead of explicit wiring, but behavior becomes harder to trace. You can push it to operations: microservices simplify individual services but operational complexity explodes. The complexity goes somewhere. It doesn't vanish. Choose consciously where you want it to hurt least. Pattern 2: Knowledge Relocation Knowledge can't be reduced, only relocated. You can't reduce what needs to be known about a system. You can only change where that knowledge lives. Take abstraction layers again: before adding an ORM, knowledge about database queries is distributed across every function that touches the database. After adding an ORM, that knowledge concentrates in the ORM layer. Total knowledge hasn't decreased. You still need to understand how queries work, how connections are managed, how errors are handled. You've just relocated the knowledge. This creates a trade-off. Distributed knowledge means each piece is simple: local context is enough to understand what's happening. But finding patterns is hard because knowledge is scattered. Global understanding requires synthesizing information from many places. Concentrated knowledge means finding answers is easy: look in the abstraction layer. But each piece is more complex: the ORM is harder to understand than any individual query was. Which distribution is better depends on your team, your system, your change patterns. When a new developer asks where logic lives, I can say "check the ORM" instead of "check twenty controllers." Same knowledge needed, better location. But now that developer needs to understand the ORM's complexity. I've seen teams struggle with this trade-off. A microservices architecture distributes knowledge across service boundaries. Each service is simpler to understand in isolation, but understanding cross-service workflows requires mental synthesis of multiple codebases. A monolith centralizes that knowledge. You can trace a request end-to-end in one codebase, but the concentration makes the monolith harder to navigate. The knowledge exists either way. The question is: where does it hurt least? If you have autonomous teams, distributing knowledge across service boundaries might work. If you have frequent cross-cutting changes, centralizing knowledge in a monolith might be better. You're not reducing knowledge. You're choosing where developers encounter it. Pattern 3: Decision Relocation Decisions can't be eliminated. Every decision must be made somewhere. Moving where decisions happen doesn't reduce total decisions. Consider configuration. You have a decision: "Which database connection string to use?" You can make it in code: if environment equals production, use this connection; otherwise use that one. Or you can make it in config: read from environment variable or config file. Same decision. Different location. Someone still decides what the database URL is. The decision moved from code to configuration. It didn't disappear. The choice of where to make decisions has consequences. Compile-time decisions mean fast runtime but slow development: changing behavior requires changing code. Runtime decisions mean slow runtime but fast iteration: change config and restart. Configuration-time decisions mean flexible behavior but configuration becomes complex: now you have configuration management, templating, validation. Convention-based decisions mean simple explicit code but you must learn the conventions. "Magic" behavior that's invisible until you know the pattern. I've debugged systems where configuration grew so complex it became code by another name. YAML files with conditionals, includes, variable substitution. Essentially a programming language without the tooling. The decisions didn't decrease; they just moved to a less maintainable place. The reverse is also true. Hard-coding decisions in code means every environment difference requires a code change. I've seen teams with many if-statements checking environment variables because they never moved decisions to configuration. Same total decisions, worse location. Feature flags are the modern version of this trade-off. You move decisions from deploy time (merge to production) to runtime (toggle in a dashboard). This gives you safety and speed. You can deploy dark and enable gradually. But you pay in testing complexity: with N flags, you have 2^N possible system states. Three flags mean eight configurations to test. Ten flags mean 1,024. The decision didn't disappear. It multiplied. Pick where decisions happen based on who needs to change them and how often. If operators need to change behavior without deploying code, configuration makes sense. If developers need to understand decision logic during debugging, code makes sense. If the decision rarely changes, hard-coding might be fine. You're not reducing decisions. You're choosing who makes them and when. Pattern 4: Failure Mode Transformation Failure modes can't be eliminated. They can only be transformed. You can't eliminate how systems fail. You can only trade failure modes you understand for failure modes you don't. Moving from synchronous to asynchronous is classic. Synchronous systems fail with timeouts, deadlocks, resource exhaustion when threads block. Asynchronous systems fail with message loss when queues drop messages, ordering issues when messages arrive out of sequence, partial failures when some operations complete and others don't. You traded known failures for different failures. Total failure surface area might even increase. I've debugged async message loss that took days to track down. With sync systems, timeouts show up immediately in logs. I'm not saying one is better. I'm saying they fail differently, and you're choosing which failure mode you'd rather debug. The same pattern appears everywhere. Move from monolith to microservices? You trade in-process call failures (immediate stack traces) for network call failures (distributed tracing, timeouts, partial failures). Move from SQL to NoSQL? You trade constraint violations (database enforces referential integrity) for data inconsistency (application must enforce integrity). I've watched teams adopt new technologies expecting them to be "more reliable," then spend months learning their failure modes. The new system wasn't less reliable. It just failed differently. And the team's existing monitoring, debugging practices, and mental models were all tuned to the old failure modes. This doesn't mean you shouldn't go async, or adopt microservices, or use NoSQL. It means recognize the trade-off. You're not eliminating failure modes: you're choosing which failure modes you'd rather handle. Maybe async failures are easier to handle in your context. Maybe you have better tools for debugging message loss than deadlocks. Maybe your team has experience with distributed systems failure modes. That's a valid trade. Just don't pretend the old failure modes disappeared: they transformed into new ones. And plan to invest in learning how the new system fails. Pattern 5: Testing Burden Relocation Testing burden can't be reduced, only relocated. You can't reduce what needs to be tested. You can only move where testing happens. Type systems are the clearest example. Without static types, you need more runtime tests because type verification happens at runtime: tests must verify both types and logic. With static types, you need fewer runtime tests because type verification happens at compile time: tests verify logic only, types are checked by the compiler. Testing effort didn't disappear. It moved from runtime tests to compile-time checks. The shift has trade-offs. Compile-time verification gives faster feedback: you know about type errors before running code. But it adds compilation overhead and can't test runtime-only behaviors like "does this API actually return the structure we expect?" Runtime testing gives slower feedback but tests actual system behavior. Same amount of verification work. Different timing. The same pattern appears with integration vs. unit tests. Heavy integration testing means you verify actual system behavior but tests are slow and brittle. Heavy unit testing with mocks means tests are fast and isolated but you need integration tests anyway to verify the mocks match reality. The testing burden didn't change. You're choosing between "test real interactions slowly" and "test mock interactions quickly plus verify mocks match." I've seen teams swing between extremes. All integration tests: comprehensive but painfully slow, so developers avoid running them. All unit tests with mocks: fast but brittle when mocks drift from reality, leading to "tests pass but production fails." The burden exists either way. The question is: where do you want verification to happen? Early in development (static types, unit tests, compile-time checks) or late in deployment (runtime tests, integration tests, production monitoring)? Each approach has different feedback loops and different failure modes. You're not reducing testing. You're choosing when you discover problems and how much machinery you need to discover them. Pattern 6: Assumption Visibility Trade-off Assumptions can't be eliminated, only made explicit or implicit. You can't reduce assumptions. You can only change their visibility. An implicit assumption looks like this: a function expects user.email to exist and be a string. The code just calls user.email.lower() and hopes. An explicit assumption documents it: add type hints, add null checks, add validation. Same assumption: user must have an email that's a string. Now it's visible instead of hidden. Implicit assumptions are cheaper to write but expensive to debug. When they're violated, you get cryptic errors: AttributeError: 'NoneType' has no attribute 'lower'. You have to trace back to figure out the assumption. Explicit assumptions are expensive to write but cheap to debug. When they're violated, you get clear errors: ValueError: User must have email. Total cost is conserved. You're choosing when to pay it: upfront with explicit checks, or later when debugging implicit assumptions. The same trade-off appears with API contracts. Implicit contracts mean less documentation, less validation code, faster development. But when clients violate expectations, you get runtime failures that are hard to diagnose. Explicit contracts mean more upfront work (OpenAPI specs, request validation, comprehensive error messages) but violations are caught immediately with clear feedback. I've debugged production issues that took hours to diagnose because assumptions were buried deep in code. "Why does this fail for some users but not others?" Eventually you discover an implicit assumption: the code assumes users have an email, but imported users from legacy systems don't. The assumption existed either way. It just wasn't visible until it broke. The question is: where do you want to pay the cost? Write explicit checks upfront (slower development, clearer debugging) or deal with implicit assumptions when they break (faster development, cryptic failures)? Neither reduces the total assumptions in your system. You're choosing whether to document them in code or discover them during debugging. Why These Patterns Matter Once I understood these relocation patterns, how I approached design changed completely. When someone proposes "simplifying" the system, the first question should be: "Where does the complexity go?" It doesn't disappear. It moves. The proposal might still be good: maybe the new location is better. But recognize it's a trade, not an elimination. This doesn't mean simplification is impossible. You can absolutely reduce total complexity: Delete dead code: If code contributes nothing to requirements (truly dead), removing it eliminates complexity. No relocation. Use better abstractions: Replace 50 lines of manual logic with 1-line library call. The library maintains complexity, but amortized across thousands of users, your system's complexity drops. Remove accidental complexity: Decouple unnecessarily entangled components. Clean up tech debt. Simplify overly complex solutions. The key: These eliminate accidental complexity. Essential complexity (what the problem inherently requires) is what relocates, not eliminates. Common examples: "Let's use microservices to simplify development." Where does complexity go? From code organization to service coordination. You trade monolith complexity for distributed system complexity. "Let's add caching to speed things up." Where does complexity go? From query performance to cache management. You trade slow queries for invalidation logic. "Let's make the API more flexible." Where does complexity go? From API code to API consumers. You trade server complexity for client complexity. These might all be good decisions. But they're trades, not improvements in absolute terms. Microservices might be the right trade if you have the team size and tooling to handle distributed systems. Caching might be right if query performance is your bottleneck and you can handle invalidation. Flexible APIs might be right if you have sophisticated clients and want to iterate server-side less often. The key is naming what's being relocated and choosing where you want it to live. Before changing anything, identify the relocating quantity: Is this complexity? Where will it move? Is this knowledge? Where will it concentrate? Is this a decision? Where will it happen instead? How to Work With These Patterns Where should complexity live? Where will it hurt least? Example: API design. You can have a complex API with simple client code, or a simple API with complex client code. Neither eliminates complexity: they distribute it differently. Complex API means server handles edge cases, versioning, validation. Clients just call simple methods. Simple API means server provides primitive operations. Clients compose them to handle edge cases. I've worked with APIs that do everything (clients love it, server team drowns) and APIs that provide primitives (clients write boilerplate but have control). Same complexity, different distribution. The complexity is conserved. Where should it live? If you have many clients, push complexity to the API: pay the cost once, save it N times. If you have few clients and a rapidly changing server, simple API with complex client code might work fine. Choose your trades consciously. You can't eliminate conserved quantities. But you can choose better locations. Moving complexity from the hot path to the cold path is usually good: cache invalidation runs less often than queries. Moving complexity from novices to experts is often good: let experienced developers handle the abstraction so junior developers use a simpler interface. Moving complexity from many places to one place is often good: centralize knowledge even if that one place becomes more complex. But measure both sides. When you move complexity, measure both the source and destination. Code complexity decreased 40%, configuration complexity increased 60%, net result is +20% total complexity. If you only measure one side, you'll think you eliminated complexity. You didn't: you relocated it, and it grew. Measure what you gained and what you paid. Accept that some things don't simplify. If you keep trying to simplify something and complexity keeps showing up elsewhere, maybe the system has inherent complexity. Some problems are just complex. No architectural cleverness eliminates their complexity. You can only distribute it more or less well. Recognizing irreducible complexity lets you stop fighting it and start managing it. What Actually Lasts But step back from the code for a moment. If everything eventually gets rewritten or deleted, what's the point of these choices? The answer: some things outlast the code. Patterns last. Design patterns outlive implementations. Separation of concerns, dependency injection, event-driven architecture: these patterns transfer across rewrites. The specific code gets replaced but the patterns persist. When you're choosing where complexity lives, you're really choosing patterns. Those patterns will outlast the code. Understanding lasts. Understanding the domain outlives the code. How the business works, what users need, why systems interact: this knowledge compounds over time. The code gets rewritten but understanding remains. When you're deciding where knowledge should live, invest in shared understanding. Documentation rots but team knowledge grows. Tests as specification last. Tests document expected behavior. They outlive implementations. When you rewrite, tests preserve requirements while code changes. The investment in test quality pays off when refactoring or replacing code. Tests preserve intent: what should this system do? Team culture lasts. How your team writes, reviews, and maintains code outlasts any particular codebase. Quality standards, review practices, testing discipline: these transfer to the next system. When you're working with these relocation patterns, you're building patterns of thinking that persist beyond the current code. Invest in culture. It compounds. The liberation comes from seeing these patterns. Once you understand that complexity relocates rather than disappears, you stop looking for solutions that eliminate it. You look for solutions that put complexity where it belongs. You measure both sides of the trade. You name what's being relocated and choose where it lives. And you invest in what actually lasts: patterns, understanding, and culture. While accepting that code is temporary. These relocation patterns aren't limitations. They're reality. You can't violate them. But you can work with them. And working with them is better than pretending they don't exist. Note: Originally published on ITNEXT: https://itnext.io/complexity-cant-be-eliminated-it-can-only-be-moved-d122f7952715</description><pubDate>Fri, 23 Jan 2026 04:09:33 +0000</pubDate><guid isPermaLink="false">https://dev.to/thesystemistsimon/complexity-cant-be-eliminated-it-can-only-be-moved-2am7</guid><source url="https://dev.to/feed">Dev.to</source><score>8.7</score></item><item><title>CTO Chris Aniszczyk on the CNCF push for AI interoperability</title><link>https://thenewstack.io/cto-chris-aniszczyk-on-the-cncf-push-for-ai-interoperability/</link><description>On a surface level, AI agents aren‚Äôt so different from microservices, according to Chris Aniszczyk, co-founder and CTO of the The post CTO Chris Aniszczyk on the CNCF push for AI interoperability appeared first on The New Stack .</description><pubDate>Thu, 22 Jan 2026 21:00:55 +0000</pubDate><guid isPermaLink="false">https://thenewstack.io/?p=22813492</guid><source url="https://thenewstack.io/feed">The New Stack</source><score>8.4</score></item><item><title>An AI Library of Production-Ready Skills for AI Agents</title><link>https://dev.to/dailyaimode/an-open-source-library-of-production-ready-skills-for-ai-agents-15ml</link><description>Hello everyone, I'm the developer behind https://agentskills.to During development, I found repeatedly crafting complex prompts for Claude Code, Cursor, or Codex CLI to be extremely time-consuming. To solve this problem, I built AgentSkills.to‚Äîan open-source ‚Äúskills‚Äù marketplace designed to make AI agents truly productive. Why give it a try? Plug-and-play: Access over 3,700 production-grade skills spanning frontend, backend, and DevOps. Cross-Platform: Seamlessly integrates with Claude Code, Cursor, Codex CLI, and Amp. Open-Source Sharing: Access all workflows for free or contribute your exclusive tricks on GitHub. Our goal is to eliminate wasted time ‚Äúreinventing the wheel‚Äù by enabling rapid delivery of high-quality code through community-validated skills. Try it out: https://agentskills.to</description><pubDate>Thu, 22 Jan 2026 23:48:50 +0000</pubDate><guid isPermaLink="false">https://dev.to/dailyaimode/an-open-source-library-of-production-ready-skills-for-ai-agents-15ml</guid><source url="https://dev.to/feed">Dev.to</source><score>8.9</score></item><item><title>Report: Apple plans to launch AI-powered wearable pin device as soon as 2027</title><link>https://arstechnica.com/apple/2026/01/report-apple-plans-to-launch-ai-powered-wearable-pin-device-as-soon-as-2027/</link><description>Apple, OpenAI, Meta, and more are all racing toward AI hardware products.</description><pubDate>Thu, 22 Jan 2026 21:32:28 +0000</pubDate><guid isPermaLink="false">https://arstechnica.com/apple/2026/01/report-apple-plans-to-launch-ai-powered-wearable-pin-device-as-soon-as-2027/</guid><source url="https://feeds.arstechnica.com/arstechnica/index">Ars Technica</source><score>9.3</score></item><item><title>An AI pin is beneath Apple</title><link>https://www.engadget.com/ai/an-ai-pin-is-beneath-apple-182744647.html?src=rss</link><description>So it's come to this: Apple is reportedly working on a wearable AI pin . According to The Information , it is going to be a small device with "multiple cameras, a speaker, microphones and wireless charging." It sounds like the perfect gadget to pair with the long-awaited AI-powered Siri update , which will also reportedly work as a chatbot . But while many Apple rumors conjure up an air of excitement, the notion of an Apple AI pin sounds downright baffling. Worse, it just seems desperate. Apple, the company known for taking its time to jump into new categories with more thoughtful solutions than its competitors, is reportedly chasing the specter of OpenAI's unreleased AI pin. Never mind that OpenAI has never actually produced any hardware, and that it arguably stumbled into its position as a leading AI player. And never mind that Humane's AI pin was a notorious failure that barely worked, and seemed pointless from the start. Sure, Apple doesn't want more AI eggs on its face, after the delay of its Siri revamp and the underwhelming (and error-prone) debut of Apple Intelligence . Beyond OpenAI, there's also competition from Meta's Ray-Ban smart glasses, which lean heavily on the company‚Äôs AI. There‚Äôs also the looming threat of whatever AI hardware Meta is cooking up next, following the layoffs from its virtual reality division . And while Google doesn‚Äôt have much to show from its Android XR platform , which aims to bring its Gemini AI to your face, Samsung‚Äôs Galaxy XR is a start. We‚Äôve also recently seen compelling demos of Google‚Äôs AR glasses prototypes and Xreal‚Äôs Project Aura glasses . If Apple's AI pin serves as a conduit to Siri, is it really that much more convenient than using an iPhone, AirPods or even an Apple Watch to do the same? The company has reportedly nixed plans to put cameras in the Apple Watch, and Bloomberg suggests it‚Äôs opting instead to focus on delivering its own smart glasses this year. But it‚Äôs not hard to imagine that faster hardware could let the Apple Watch handle more Siri and AI-related tasks on its own. It‚Äôs already a fairly self-sufficient device, allowing you to ask basic Siri queries, run apps and listen to music without an iPhone ‚Äî the cellular models are even more capable since they can take calls and send messages. Rumors also point to infrared cameras coming to the next AirPods and AirPod Pros . Instead of taking photos, they could enable hand gestures and environmental awareness, which might be useful for Apple Intelligence down the line. The addition of heart rate tracking in the AirPods Pro 3 shows that there are still new features Apple can bring to its buds, beyond listening to music. At best, an Apple AI pin could just be a simple way for someone to access Siri if they don‚Äôt want to wear an Apple Watch, plug in AirPods or have their iPhone within shouting distance. But at least those devices do other things beyond talking to Siri. The same is true for Meta‚Äôs Ray-Bans and future smart glasses. Even without accessing AI, they‚Äôll still let you listen to music, take calls and, well, be glasses for those who need prescription frames. Given the vocal pushback against Meta's Ray-Ban smart glasses , which are also being banned on cruises , clubs and other venues, I'm also not convinced many people would be eager to prominently display a surveillance device throughout the day. Wired‚Äôs Julian Chokkattu was questioned about wearing a camera while he was testing the Humane AI Pin, and I‚Äôve also had to explain to curious people why I was wearing Xreal‚Äôs smart glasses, which feature a prominent camera accessory. Sure, we're already living in a panopticon of smartphone cameras, but it's also obvious when someone is using their phone to capture photos and video. An AI pin just dangling off of your clothes is a constant threat, an unblinking eye. Even if Apple implements some sort of capture notification, someone will always try to circumvent it . While The Information notes Apple's AI pin may never actually see the light of day, I wouldn't be surprised if it does. This is the company that partnered with OpenAI just to make Siri appear slightly smarter with the debut of Apple Intelligence. And instead of building its own home-brewed AI models, it's banking on Google's Gemini to power Siri's big AI upgrade , as well as its future foundation models. When it comes to AI, Apple will do almost anything to avoid being seen as a straggler (and to avoid even more stock declines ). It‚Äôs genuinely strange that Apple, the company that let Samsung and Google get a multi-year head start on foldable smartphones and hasn't yet jumped into the world of smart rings, could fast-track an AI pin for 2027. It‚Äôs yet another example of how the AI hype cycle has warped priorities throughout the tech industry. But at least Apple‚Äôs fortunes don‚Äôt depend on standalone AI hardware as much as OpenAI. This article originally appeared on Engadget at https://www.engadget.com/ai/an-ai-pin-is-beneath-apple-182744647.html?src=rss</description><pubDate>Thu, 22 Jan 2026 18:27:44 +0000</pubDate><guid isPermaLink="false">6a9c4806-23e0-4b7e-8496-a6afa6f6c483</guid><source url="https://www.engadget.com/rss.xml">Engadget</source><score>9.6</score></item><item><title>Why agentic LLM systems fail: Control, cost, and reliability</title><link>https://thenewstack.io/why-agentic-llm-systems-fail-control-cost-and-reliability/</link><description>In the past few years, agentic AI systems like AutoGPT, BabyAGI and others have demonstrated, through prompting, that large language The post Why agentic LLM systems fail: Control, cost, and reliability appeared first on The New Stack .</description><pubDate>Thu, 22 Jan 2026 18:00:45 +0000</pubDate><guid isPermaLink="false">https://thenewstack.io/?p=22813323</guid><source url="https://thenewstack.io/feed">The New Stack</source><score>8.7</score></item><item><title>The head chef model for AI-assisted development</title><link>https://thenewstack.io/the-head-chef-model-for-ai-assisted-development/</link><description>As AI coding assistants become more capable, the relationship between developers and their tools is evolving beyond a simple autocomplete. The post The head chef model for AI-assisted development appeared first on The New Stack .</description><pubDate>Thu, 22 Jan 2026 19:00:07 +0000</pubDate><guid isPermaLink="false">https://thenewstack.io/?p=22813371</guid><source url="https://thenewstack.io/feed">The New Stack</source><score>9.7</score></item><item><title>Building Agents for Production, Designing Multi-Agent Applications, Tonight‚Äôs Atlanta Meetup, and‚Ä¶</title><link>https://odsc.medium.com/building-agents-for-production-designing-multi-agent-applications-tonights-atlanta-meetup-and-09ec12c2e87b?source=rss-2b9d62538208------2</link><description>Building Agents for Production, Designing Multi-Agent Applications, Tonight‚Äôs Atlanta Meetup, and Bridging Innovation and Practicality AI didn‚Äôt change overnight. It evolved ‚Äî step by step. At ODSC AI East, you‚Äôll learn how leading teams are designing, deploying, and managing AI systems that work alongside humans ‚Äî across engineering, data science, and the enterprise. Register Now for 50% Off! How AI Helps Reduce False Alarms in Security Systems Traditional alarms excel at detecting change, but they struggle to interpret context. AI fills that gap by learning patterns, correlating multiple signals, and making higher-quality decisions before escalating an alert. Building AI Agents That Work in Production: Core Fundamentals for Junior Engineers Most AI agents fail outside demos. Learn how to build production-ready AI agents by mastering LLM limits, evaluation, state, guardrails, and system design. Course of the Week: Design and Build Multi-Agent Applications This session explores how orchestration frameworks like LangGraph ‚Äî alongside emerging open protocols such as the Model Context Protocol (MCP) and Agent-to-Agent (A2A) ‚Äî enable scalable, interoperable multi-agent systems. Video of the Week: Bridging Innovation and Practicality with Dell AI Factory and NVIDIA by Helen O‚ÄôSullivan This session explores how the Dell AI Factory with NVIDIA empowers enterprises to overcome challenges like skill gaps, data management, and security concerns. ODSC Highlights The Agentic AI Summit is Now Live! Week 1 of the Agentic AI Summit is currently live. You can catch these videos on demand tomorrow to catch up before next week‚Äôs sessions devoted to implementation. Build AI Systems Without a CS Degree: The ODSC AI East 2026 Bootcamp ODSC AI East 2026 AI Bootcamp features eight weekly sessions that cover the technical concepts needed to work with AI systems confidently. Introducing the AI X Leadership Summit at ODSC AI East 2026 ‚Äî Your Executive Edge in AI The AI X Leadership Summit at ODSC AI East 2026 brings together executives to explore AI strategy, leadership, and its real-world enterprise impact. Leadership AI Skills for Managers in 2026: 7 Essentials for Leading in an AI-Driven Organization AI is now a core management responsibility. Learn the 7 essential AI skills for managers in 2026, from cost and risk to evaluation, culture, and leadership. Ai X Leadership Summit April 28‚Äì29 | Boston Co-located with ODSC AI East, this leadership-focused experience is designed for those navigating real AI adoption, featuring cross-industry panels, highly curated roundtables, and honest conversations about ROI, risk, and readiness. Become a Leader in AI Data Science &amp; AI News Salesforce Rebuilds Slackbot Into a Claude-Powered AI Agent for Enterprise Search and Actions Salesforce has released a rebuilt Slackbot for Slack Business+ and Enterprise+ plans, using Anthropic‚Äôs Claude to answer questions and more. Claude in Microsoft Foundry Adds Healthcare and Life Sciences Tools For Agentic, Regulated Workflows Anthropic is adding healthcare and life sciences connectors and agent skills to Claude in Microsoft Foundry. McKinsey Asks Graduates to Use AI Chatbot in Recruitment Process McKinsey is piloting an ‚ÄúAI interview‚Äù for some graduate applicants, asking finalists to collaborate with its internal chatbot Lilli. Gemini Introduces Personal Intelligence to Connect Gmail, Photos, and More Google has launched Gemini Personal Intelligence beta for AI Pro and AI Ultra subscribers, enabling opt-in connections to popular apps. Wikipedia Marks 25 Years With New AI Licensing Deals to Manage Bot Traffic and Fund Infrastructure Wikipedia is signing new licensing deals with major AI companies to monetize high-volume access and reduce server strain from bots. New Podcast Episode: Building Trustworthy Voice AI: The Evaluation Playbook with Brooke Hopkins In this episode, we speak with Brooke Hopkins, Founder and CEO of Coval, to unpack why Voice AI reliability is starting to look a lot like the reliability problem self-driving teams have been solving for years. Spotify | Apple | SoundCloud Upcoming Webinars, Meetups, and Ai+ Live Training Sessions ODSC AI &amp; Snowflake Meetup in Atlanta, GA Thursday, January 22nd, 6:00 PM ‚Äî 8:00 PM ET ‚ÄãODSC AI is back in Atlanta! We host data science communities across the globe, and now it‚Äôs time to bring our local experts together for an evening of hands-on AI development. ODSC AI &amp; Chalk Meetup in San Francisco Thursday, January 29th, 6:00 PM ‚Äî 8:00 PM PST ‚ÄãWhen systems rely on rigid batch pipelines and precomputed features, the information is often outdated by the time a ranking decision is actually made. Join us in San Francisco for an evening dedicated to solving the production bottlenecks of real-time machine learning.</description><pubDate>Thu, 22 Jan 2026 17:02:06 GMT</pubDate><guid isPermaLink="false">https://medium.com/p/09ec12c2e87b</guid><source url="https://medium.com/feed/@odsc">ODSC</source><score>8.3</score></item><item><title>Focus Restore feature for your Cursor</title><link>https://dev.to/alexey_elizarov_963bfdae8/restoring-focus-to-cursor-after-agent-completion-256p</link><description>When working with Cursor agents, I noticed a small but recurring productivity leak. While the agent is running, it‚Äôs very easy to switch context ‚Äî read a website, check Telegram, do something else. The problem appears when the agent finishes: Cursor doesn‚Äôt automatically regain window focus, and I often return to it with a delay. This breaks the flow. To solve this, I built a small utility hook that automatically brings the Cursor window back into focus once the agent finishes its work. What it does ‚Ä¢ Listens for agent completion ‚Ä¢ Activates the Cursor window automatically ‚Ä¢ Helps you immediately continue working without context switching friction Key points ‚Ä¢ Cross-platform (macOS, Windows, Linux) ‚Ä¢ Lightweight and minimal ‚Ä¢ Designed specifically as a UX improvement for agent-based workflows ‚Ä¢ Easy to install and remove Why this matters When you use agents frequently, even small delays add up. This hook doesn‚Äôt try to be ‚Äúsmart‚Äù ‚Äî it just removes a tiny but annoying interruption in the feedback loop between you and the agent. Sometimes that‚Äôs all you need. Repository üëâ https://github.com/beautyfree/cursor-window-activate-hook If you‚Äôre using Cursor agents heavily and notice the same issue ‚Äî feel free to try it out or adapt it to your workflow. Feedback and improvements are welcome.</description><pubDate>Thu, 22 Jan 2026 19:39:14 +0000</pubDate><guid isPermaLink="false">https://dev.to/alexey_elizarov_963bfdae8/restoring-focus-to-cursor-after-agent-completion-256p</guid><source url="https://dev.to/feed">Dev.to</source><score>9.6</score></item><item><title>Humans&amp; thinks coordination is the next frontier for AI, and they‚Äôre building a model to prove it</title><link>https://techcrunch.com/2026/01/22/humans-thinks-coordination-is-the-next-frontier-for-ai-and-theyre-building-a-model-to-prove-it/</link><description>Humans&amp;, a new startup founded by alumni of Anthropic, Meta, OpenAI, xAI, and Google DeepMind, is building the next generation of foundation models for collaboration, not chat.</description><pubDate>Thu, 22 Jan 2026 19:24:13 +0000</pubDate><guid isPermaLink="false">https://techcrunch.com/?p=3085222</guid><source url="https://techcrunch.com/feed/">TechCrunch</source><score>8.2</score></item><item><title>Google DeepMind CEO is ‚Äòsurprised‚Äô OpenAI is rushing forward with ads in ChatGPT</title><link>https://techcrunch.com/2026/01/22/google-deepmind-ceo-is-surprised-openai-is-rushing-forward-with-ads-in-chatgpt/</link><description>Google DeepMind CEO Demis Hassabis says the tech giant isn't pressuring him to insert ads into the AI chatbot experience.</description><pubDate>Thu, 22 Jan 2026 19:41:01 +0000</pubDate><guid isPermaLink="false">https://techcrunch.com/?p=3085101</guid><source url="https://techcrunch.com/feed/">TechCrunch</source><score>8.4</score></item><item><title>Getting Started with Gemini 3: Deploy Your First Gemini 3 App to Google Cloud Run</title><link>https://cloud.google.com/blog/topics/developers-practitioners/getting-started-with-gemini-3-deploy-your-first-gemini-3-app-to-google-cloud-run/</link><description>The previous blog demonstrated how simple it is to start using Gemini 3. With Google AI Studio , you can easily continue your journey by getting an API Key and ready-to-use code. This allows you to quickly run and then customize your code to build your next application. In this blog, we will show you how to vibe code your first app‚Äîwhich leverages the Gemini 3 Flash Preview model ‚Äî and deploy it as a publicly accessible URL on Google Cloud Run. Google AI Studio lets you go from idea to app quickly by using natural language to generate fully functional apps using the power of Gemini 3. In Build mode, you can describe the application that you want by typing into the input box, or using the speech-to-text button. Once the app is ready, you can deploy it to Google Cloud Run with a single button click, allowing you to go from code to shareable URL in seconds. Cloud Run is a fully managed platform where you can run frontend and backend services, batch jobs, host LLMs, and queue processing workloads without the need to manage infrastructure. What's more, you get two million requests free per month on Cloud Run. Let's get started! Step 1: Vibe Code in Build Mode The Build mode in Google AI Studio lets you quickly build (or vibe code) and deploy apps that test out the latest capabilities of Gemini like Nano Banana and the Live API . Once logged into Google AI Studio , click on " Build " in the menu on the left, and then type into the input box the idea for your application, describing it in as much detail as possible. Once you are ready, click on " Build " within the text area. Ensure that you select a Gemini 3 model for building this out - you can select Gemini 3 Flash Preview model - this is a model in the Gemini 3 family with a good balance of quality and speed. The prompt we used in our example is: ``` Build me a game - "Where am I"? Come up with an interesting one line popular fact about a popular tourist city. Give me 4 multiple choice options for the city. Score me on 5 such questions. Use gemini-3-flash-preview to generate the questions and the multiple choice answers. ``` Now sit back and watch Gemini understand your request and build out a full featured and working app. While you wait, Google AI Studio suggests prompts for additional features on the panel on the right, which you could use to enhance your application. Step 2 : Test the App A great feature of Google AI Studio is that you can test the app without leaving it. As soon as the app is generated, it is available immediately in the "Preview" panel on the right. Here you can test your app, check that the functionality works as intended. You can even iterate on the features of the app - either using the AI suggested enhancements or adding your own features. Step 3 : Deploy the App Once the app is ready to your liking, AI Studio makes it very easy to deploy the app to Cloud Run on your Google Cloud Project in just a few clicks. Note: To deploy to Cloud Run, you will need a Google Cloud Project with a valid billing account. If this is your first time with Google Cloud, follow the instructions in this codelab to create your first project with a trial billing account. Using the "Deploy App" button on the top right, select the project that you would like to deploy it to (import the project first, if it is the first time you are deploying to that project from AI Studio), click "Deploy app" and wait a few minutes while the app you just Vibe Coded is deployed and ready to be shared. Once you get the confirmation of deployment, you are presented with an App URL where the app is hosted and you can use this URL to access your application powered by Gemini 3 and hosted on Google Cloud. Google AI Studio exports the current API Key that you are using to Google Cloud Run's environment variables so that your application is able to access the Gemini API. If you have to change the API key, you can do that easily in the Service details section within Cloud Run within the Google Cloud console . Congratulations! You have now built an application powered by Gemini 3 from the ground up using Google AI Studio and deployed it on Google's highly scalable infrastructure from where you can share your innovative ideas with the world. In the next blog post , you‚Äôll learn how to quickly vibe-code a full app and deploy it to Google Cloud.</description><pubDate>Thu, 22 Jan 2026 10:02:00 +0000</pubDate><guid isPermaLink="false">https://cloud.google.com/blog/topics/developers-practitioners/getting-started-with-gemini-3-deploy-your-first-gemini-3-app-to-google-cloud-run/</guid><source url="https://cloudblog.withgoogle.com/rss">Google Cloud Blog</source><score>8.9</score></item><item><title>How Google SREs Use Gemini CLI to Solve Real-World Outages</title><link>https://cloud.google.com/blog/topics/developers-practitioners/how-google-sres-use-gemini-cli-to-solve-real-world-outages/</link><description>One of our favorite mottos in Google Site Reliability Engineering (SRE) is: "Eliminate Toil." You hear it in the microkitchens in Zurich and the hallways in Mountain View. This motto refers to the SRE mission of replacing repetitive, manual work with engineered systems. But as a senior SRE once explained, this doesn't just mean writing a script to solve a problem once. It means building the automation that triggers that script at the exact right moment‚Äîoften the hardest part. AI has already revolutionized how we write code, but what about how we operate it? Can AI safely solve operational problems? Can it assist operators during a high-pressure outage without taking away control? In this article, we‚Äôll delve into real scenarios that Google SREs are solving today using Gemini 3 (our latest foundation model) and Gemini CLI ‚Äîthe go-to tool for bringing agentic capabilities to the terminal. The Scenario: Fighting "Bad Customer Minutes" Meet Ram√≥n. Ram√≥n is a Core SRE , meaning he works in the engineering group that develops the foundational infrastructure for all of Google's products: safety, security, account management, and data backends for multiple services. When infrastructure at this layer has a hiccup, it‚Äôs visible across a massive range of products immediately. Speed is vital, and we measure it in Bad Customer Minutes . Every minute the service is degraded burns through our Error Budget. To combat this, we obsess over MTTM (Mean Time to Mitigation) . Unlike Mean Time to Repair (MTTR), which focuses on the full fix, MTTM is about speed: how fast can we stop the pain? In this space, SREs typically have a 5-minute Service Level Objective (SLO) just to acknowledge a page, and extreme pressure to mitigate shortly after. Our incident usually follows four distinct stages: Paging: The SRE gets alerted. Mitigation: We "stop the bleeding" to reduce Bad Customer Minutes , often before we even know why it broke. Root Cause: Once users are happy, we investigate the underlying bug and fix it for good. Postmortem: We document the incident and add extensive action items on engineering teams, that are prioritised to ensure it never happens again. Let's walk through a real (simulated) outage to see how Gemini CLI accelerates every step of this journey to keep MTTM low. Step 1: Paging and Initial Investigation Context: It's 11:00 AM. Ram√≥n‚Äôs pager goes off for incident s_e1vnco7W2 . When a page comes in, the clock starts ticking. Our first priority isn't to fix the code‚Äîit's to mitigate the impact on users. Thanks to the extensive work on Generic Mitigations by Google SREs, we have a defined, closed, set of standard classes of mitigations (e.g., drain traffic, rollback, restart, add capacity). This is a perfect task for an LLM: classify the symptoms and select a mitigation playbook. A mitigation playbook is an instruction created dynamically for an agent to be able to execute a production mutation safely. These playbooks can include the command to run, but also instructions to verify that the change is effectively addressing the problem, or to rollback the change. Ram√≥n opens his terminal and uses Gemini CLI. Gemini immediately calls the fetch_playbook function from ProdAgent (our internal agentic framework). It chains together several tools to build context: get_incident_details : Fetches the alert data from our Incident Response Management system (description, metadata, prior instances, etc). causal_analysis : Finds causal relations between different time series behaviour and generic mitigation labels. timeseries_correlation : Finds pairs of time series that are correlated, which may help the agent find root causes and mitigation log_analysis : Uses log patterns and volumetric analysis to determine anomalies on the stream of logs from the service. Based on the symptoms, Gemini recommends the borg_task_restart playbook (analogous to a Kubernetes pod restart) as the immediate mitigation . It knows exactly which variables to fill based on the alert context. Gemini proposes the following action: Ram√≥n reviews the plan; it looks solid. He types: "SGTM, execute the restart." Keeping the human in the loop is important at the moment, to validate that mitigations that are going to be actuated make sense and are safe to apply to the system. In the future this will change, as we build confidence in the agents and their capabilities and new agentic safety systems. Step 2: The Mitigation (Stopping the Bleeding) Safety Check: Copilot, Not Autopilot Before we execute, we need to address safety. We cannot simply let an autonomous agent operate autonomously on production infrastructure: a command that is safe under some conditions, or system state, may not be safe in other situations (for example: a binary rollback may generally be safe, but not when a service is receiving a configuration push). This is where the CLI approach shines. It implements a multi-layer safety strategy to ensure the agent acts as a responsible copilot, not an autopilot . Deterministic Tools: The agent doesn't write random bash scripts; it selects from strictly typed tools (via Model Context Protocol) like borg_task_restart . Risk Assessment: Every tool definition includes metadata about its potential impact (e.g., safe, reversible, destructive). The system automatically flags high-risk actions for stricter review. Policy Enforcement: Even if the agent tries to execute a valid command, a policy layer checks if it's allowed in the current context (e.g., "No global restarts during peak traffic" or "Requires 2-person approval"). Human-in-the-Loop: Finally, the CLI forces a confirmation step. The agent proposes the mutation, but Ram√≥n authorizes it. This allows us to move at AI speed while maintaining human accountability. Audit Trails: Because every action is proxied through the CLI, we automatically log exactly what the AI proposed and what the human approved, satisfying compliance requirements. The Execution Moving from theory to practice is where things often get messy. In this instance, the restart fails. In a manual world, this is where MTTM increases significantly. The operator has to context switch, open new dashboards, manually grep logs, and lose precious minutes while Bad Customer Minutes accumulate. Instead, Gemini CLI catches the error and stays in the flow. It immediately analyzes the failure and notices a pattern: Our job is the only one in the cell failing; others are healthy. This insight is crucial. It suggests the issue isn't the cluster, but our specific binary or configuration. Because Gemini surfaced in seconds rather than minutes, we avoid chasing red herrings. The "stop the bleeding" phase is paused, and we move to investigation with minimal time lost. Step 3: Root Cause and Long-Term Fix Now that we know a simple restart won't work, we need to find the bug. Since the infrastructure is healthy, the defect must be in the application logic. Ram√≥n points Gemini to the source code. Since Google uses a massive monorepo , being able to pass a specific folder as context is powerful. "Check the changes in /path/to/service/..." Gemini starts fetching files, analyzing recent changes, and cross-referencing them with the production logs it pulled earlier. In under two minutes, it finds the culprit: a logic error in a recent configuration push. Ram√≥n asks: "Can you make a CL fixing that issue?" (Note: A CL, or Changelist, is the Google equivalent of a GitHub Pull Request). Gemini generates the patch, creating a CL that reverts the bad configuration and applies a safeguard. Gemini provides the instructions: Review and approve the CL. Submit the CL. Wait for the automated rollout. The code is fixed, the rollout begins, and the service recovers. Step 4: The Postmortem The fire is out, but the work isn't done. SRE culture is built on the Postmortem : a blameless document that analyzes what went wrong so we can learn from it. Writing postmortems can be tedious‚Äîgathering timestamps, linking logs, and recalling specific actions. Gemini CLI automates this with a Custom Command . Enter Riccardo : ‚ÄúRicc‚Äù has worked with Google Cloud SREs for years, declared a few public outages and contributed to many Post Mortems; as a Developer Advocate, he also preaches about the Art of the Post Mortem , Art of SLOs , and Incident Management to Operators. He authored a Custom Command within ProdAgent which helps create Post Mortems, build timelines and Action Items. Ram√≥n runs his postmortem command. The tool: Scrapes the conversation history, metrics, and logs from the incident. Populates a CSV timeline of all relevant events. Generates a Markdown document based on our standard SRE Postmortem template. Suggests Action Items (AIs) to prevent reoccurrence. Finally, Gemini uses the Model Context Protocol (MCP) to interact with our issue tracker: Create Bugs: It files the Action Items as real bugs in the issue tracker. Assign Owners: It assigns them to the relevant engineers. Export Doc: It pushes the final Postmortem to Google Docs. Conclusion We just walked through a full incident lifecycle‚Äîfrom a chaotic 3 AM page to a finalized Postmortem‚Äîdriven entirely from the terminal. By using Gemini CLI, we connected the reasoning capabilities of Gemini 3 with real-world operational data. We didn't just ask a chatbot for advice; we used an agent to execute tools, analyze live logs, generate a code patch, and roll it out to production. This direct integration is what allows us to aggressively reduce MTTM and minimize Bad Customer Minutes . While this story used some Google-internal tools, the pattern is universal. You can build this workflow today: Gemini CLI is open for everyone. MCP Servers allow you to connect Gemini to your own tools (Grafana, Prometheus, PagerDuty, Kubernetes, ..). Custom Commands let you automate your specific team workflows, just like our Postmortem generator . The Virtuous Cycle Perhaps the most exciting part is what happens next. That Postmortem we just generated? It becomes training data. By feeding past Postmortems back into Gemini, we create a virtuous loop of self-improvement: the output of today‚Äôs investigation becomes the input for tomorrow‚Äôs solution. Check out geminicli.com to find extensions for your stack and start eliminating toil.</description><pubDate>Thu, 22 Jan 2026 14:02:00 +0000</pubDate><guid isPermaLink="false">https://cloud.google.com/blog/topics/developers-practitioners/how-google-sres-use-gemini-cli-to-solve-real-world-outages/</guid><source url="https://cloudblog.withgoogle.com/rss">Google Cloud Blog</source><score>8.4</score></item><item><title>Want to lower your electric bill? 3 tools I use to find the biggest power hogs in my house</title><link>https://www.zdnet.com/article/power-meter-tools-for-energy-consumption-testing/</link><description>There's nothing like seeing in real time how much something is costing you to make you want to turn it off.</description><pubDate>Thu, 22 Jan 2026 15:12:39 GMT</pubDate><guid isPermaLink="false">153c3095-2991-4a73-a7dc-7f5578bd3740</guid><source url="https://www.zdnet.com/topic/big-data/rss.xml">ZDNet Big Data</source><score>9.2</score></item><item><title>Open Notebook: A True Open Source Private NotebookLM Alternative?</title><link>https://www.kdnuggets.com/open-notebook-a-true-open-source-private-notebooklm-alternative</link><description>Open Notebook is an open-source, AI-powered platform designed to help users take, organize, and interact with notes while keeping full control over their data.</description><pubDate>Thu, 22 Jan 2026 15:00:04 +0000</pubDate><guid isPermaLink="false">https://www.kdnuggets.com/?p=201109</guid><source url="https://www.kdnuggets.com/feed">KDnuggets</source><score>8.3</score></item><item><title>AI Mode in Google search can now pull context from your other apps</title><link>https://www.engadget.com/ai/ai-mode-in-google-search-can-now-pull-context-from-your-other-apps-160000103.html?src=rss</link><description>After adding Personal Intelligence to Gemini as an opt-in experience, Google has announced that it‚Äôs also integrating the feature into AI Mode in Search. What Personal Intelligence does is pull information from your Google apps to tailor its responses based on your history and interests. For Search, in particular, you can allow Personal Intelligence to look for information in your Gmail accounts and Google Photos libraries. If you use AI Mode to shop for clothes with the new feature enabled, for instance, Google could recommend items or models from a brand you previously purchased from. If it sees plane tickets or other reservations in Gmail, Google could also recommend specific items based on your destination and the season if you‚Äôre clothes shopping for that trip. Personal Intelligence is powered by Google‚Äôs Gemini 3 AI model. The company says it doesn‚Äôt train its models using information from your Gmail inbox or Google Photos library, but it does use your prompts and AI Mode‚Äôs responses. Google also warned that sometimes, the feature‚Äôs recommendations could feel inaccurate because it could not fully comprehend the context or could make incorrect connections between separate topics. At the moment, Personal Intelligence is an experimental feature that‚Äôs rolling out in Labs starting today. Google AI Pro and Ultra subscribers in the US, who use the service in English, will automatically have access to it and be able to connect AI Mode to Gmail and Google Photos. It will only be available to personal Google accounts, however, and not for Workspace accounts just yet. This article originally appeared on Engadget at https://www.engadget.com/ai/ai-mode-in-google-search-can-now-pull-context-from-your-other-apps-160000103.html?src=rss</description><pubDate>Thu, 22 Jan 2026 16:00:00 +0000</pubDate><guid isPermaLink="false">7ab4f3a7-8383-4d59-803d-d30fc992883c</guid><source url="https://www.engadget.com/rss.xml">Engadget</source><score>9.2</score></item><item><title>How Precog adds business context to make enterprise data AI-ready</title><link>https://thenewstack.io/how-precog-adds-business-context-to-make-enterprise-data-ai-ready/</link><description>Extracting data from enterprise tools like Salesforce, Ariba, or NetSuite is relatively easy. Making that data usable for AI models The post How Precog adds business context to make enterprise data AI-ready appeared first on The New Stack .</description><pubDate>Thu, 22 Jan 2026 14:00:14 +0000</pubDate><guid isPermaLink="false">https://thenewstack.io/?p=22813409</guid><source url="https://thenewstack.io/feed">The New Stack</source><score>8.6</score></item><item><title>Agentic AI ‚Äî From Workflows to Goal-Driven Systems</title><link>https://dev.to/sandeepsinghhub/agentic-ai-from-workflows-to-goal-driven-systems-423b</link><description>Agentic AI ‚Äî From Workflows to Goal-Driven Systems From rigid automation to systems that think in loops. Automation vs Intelligence (A Quick Reset) Most systems we build today are automated, not intelligent. Even many ‚ÄúAI-powered‚Äù systems still follow this model: Trigger Rule Action Exit They may use machine learning or LLMs at one step, but the control flow itself remains fixed. Agentic AI changes the control model. The Core Idea: The Agent Loop At the heart of Agentic AI is a continuous decision loop: goal ‚Üí perceive ‚Üí reason ‚Üí act ‚Üí observe ‚Üí refine This loop runs until the goal is achieved, abandoned, or the system is stopped. Unlike workflows, the loop does not assume a predefined path. A Real Example: Ecommerce Order Fulfillment Let‚Äôs ground this in a real system most developers recognize. The goal is simple: Deliver the customer‚Äôs order on time at minimum cost. That‚Äôs the only instruction. No workflow. No step-by-step logic. Goal: Defining the Outcome The agent starts with: ‚Ä¢ A clear goal (deliver on time) ‚Ä¢ Constraints (cost, SLA, inventory, location) The goal stays the same even when the environment changes. In traditional automation, workflows break. In agentic systems, plans change ‚Äî goals don‚Äôt. Perceive: Understanding the Current State The agent observes the environment: ‚Ä¢ Inventory across warehouses ‚Ä¢ Customer location ‚Ä¢ Courier availability and SLAs ‚Ä¢ Current time and delivery deadline This is not a rule check. It is situational awareness. Reason: Selecting the Best Next Action The agent asks: ‚ÄúWhat action moves me closest to the goal right now?‚Äù Possible decisions include: ‚Ä¢ Choosing a nearer warehouse ‚Ä¢ Splitting shipments ‚Ä¢ Switching couriers due to SLA risk ‚Ä¢ Upgrading shipping proactively There is no fixed order. Act: Executing Through Capabilities The agent executes actions through tools: ‚Ä¢ Inventory allocation APIs ‚Ä¢ Courier booking systems ‚Ä¢ Shipping label generation ‚Ä¢ Customer notifications These are capabilities, not scripted steps. Observe: Closing the Feedback Loop After acting, the agent evaluates: ‚Ä¢ Shipment acceptance ‚Ä¢ Delivery risk ‚Ä¢ Inventory changes Feedback keeps the system grounded in reality. Refine: Adapting to Change When conditions change ‚Äî and they always do ‚Äî the agent loops again. A courier delay does not cause failure. The agent re-evaluates, adjusts its plan, and continues toward the goal. The goal never changes. The plan does. How This Differs from Traditional Automation Traditional automation designs paths: if A ‚Üí do B else ‚Üí do C Agentic AI designs decision systems: goal + state ‚Üí best next action Automation is predictable. Agentic systems are adaptive. Where LLMs, RAG, and MCP Fit In Agentic AI does not replace these components ‚Äî it orchestrates them. LLMs provide reasoning. RAG provides grounding. Tools enable action. Memory or MCP maintains continuity. None of these are agentic alone. They become agentic when placed inside a goal-driven feedback loop. The Developer Mindset Shift You stop designing flows. You start designing: ‚Ä¢ Goals ‚Ä¢ Constraints ‚Ä¢ Capabilities ‚Ä¢ Feedback This shift reduces complexity where it matters most: control logic. Why This Matters Now Agentic systems are emerging because: ‚Ä¢ Real-world environments are unpredictable ‚Ä¢ Rules do not scale ‚Ä¢ Adaptation is required This is not hype. It is a response to complexity.</description><pubDate>Thu, 22 Jan 2026 16:07:40 +0000</pubDate><guid isPermaLink="false">https://dev.to/sandeepsinghhub/agentic-ai-from-workflows-to-goal-driven-systems-423b</guid><source url="https://dev.to/feed">Dev.to</source><score>8.4</score></item><item><title>Using coding agents, if you're not technical</title><link>https://www.bensbites.com/p/using-coding-agents-if-youre-not</link><description>Vibe code with me today</description><pubDate>Thu, 22 Jan 2026 14:52:55 GMT</pubDate><guid isPermaLink="false">https://www.bensbites.com/p/using-coding-agents-if-youre-not</guid><source url="https://www.bensbites.com/feed">Ben's Bites</source><score>8.1</score></item><item><title>Spotify brings AI-powered Prompted Playlists to the U.S. and Canada</title><link>https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/</link><description>Now available in the US and Canada, Spotify's AI-powered Prompted Playlists let users describe what they want to hear using natural language commands.</description><pubDate>Thu, 22 Jan 2026 14:00:00 +0000</pubDate><guid isPermaLink="false">https://techcrunch.com/?p=3084124</guid><source url="https://techcrunch.com/feed/">TechCrunch</source><score>8.6</score></item><item><title>Google reportedly snags up team behind AI voice startup Hume AI</title><link>https://techcrunch.com/2026/01/22/google-reportedly-snags-up-team-behind-ai-voice-startup-hume-ai/</link><description>Google has hired the CEO and top top behind voice AI startup Hume AI, signaling that voice is increasingly becoming the preferred interface over screens.</description><pubDate>Thu, 22 Jan 2026 15:12:51 +0000</pubDate><guid isPermaLink="false">https://techcrunch.com/?p=3084994</guid><source url="https://techcrunch.com/feed/">TechCrunch</source><score>8.2</score></item><item><title>Agent Factory Recap: Antigravity and Nano Banana Pro with Remik</title><link>https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-antigravity-and-nano-banana-pro-with-remik/</link><description>In Episode #17 of the Agent Factory podcast , we step away from the purely theoretical and get our hands dirty with the latest developer tools from Google. Together with Vlad, we take a deep dive into Antigravity and Nano Banana Pro, demonstrating how to build AI agents that bridge the gap between code generation and high-fidelity media. This post guides you through the key ideas from our conversation. Use it to quickly recap topics or dive deeper into specific segments with links and timestamps . Antigravity - a new agentic mission control Timestamp: 02:02 Antigravity is Google‚Äôs new agent-first development application. It is designed as a multi-window IDE that uses Gemini 3 under the hood to manage complex, asynchronous coding tasks. Unlike a standard text editor, Antigravity features a dedicated Agent Manager view where developers can interact with agents through planning modes, artifact reviews, and built-in browsers for live UI testing. Nano Banana Pro and why it‚Äôs so special Timestamp: 03:47 The model behind our slide generator is Nano Banana Pro (the technical name is Gemini 3 Pro Image model). What sets Nano Banana Pro apart is its ability to "think" before it creates. It uses Google Search grounding to retrieve real-time data, like current weather or live stock charts, and integrates that information into the generated image. The Factory Floor The Factory Floor is our segment for getting hands-on. Here, we moved from high-level concepts to practical code with live demos. Agent Architecture Timestamp: 06:41 We dove into the architecture for building the slide generator agent using a combination of the Agent Development Kit (ADK) , Antigravity , and Nano Banana Pro . Moreover we explained how these components interconnect to support the flow from a user prompt to a high-fidelity presentation by leveraging a Model Context Protocol (MCP) server. Agent Starter Pack Timestamp: 10:04 We started by introducing Agent Starter Pack and using it to spin up a new ADK project. Vibe Coding a Slide Generator Agent Timestamp: 14:56 Using Antigravity‚Äôs "Always Review" mode, we submitted a prompt to build a completely new slides agent which will use a MCP server to create detailed images. We highlighted how Antigravity handles tasks and implementation plan reviews, allowing the developer to modify the agent's proposed steps before it executes them. We also dove into automated testing with Antigravity‚Äôs web browser plugin functionality. Results Timestamp: 32:25 The slides generator agent successfully called the MCP tools to generate a series of images stored in a Cloud Storage bucket, presenting the final links directly in the UI. Antigravity reviewed the results and to our surprise immediately suggested additional improvements. Conclusion We are moving into an era where building an agent isn't just about the LLM; it's about the orchestration of tools, planning, and high-fidelity output. Antigravity is our agent manager and Nano Banana Pro acts as an asset creator. We can‚Äôt wait to see what our viewers create next with these powerful tools! Resources and links Antigravity ‚Üí https://goo.gle/3XMvGwf Nano Banana Pro ‚Üí https://goo.gle/3KMmKUG Agent starter pack ‚Üí https://goo.gle/4oLfypT Agent Development Kit (Python) ‚Üí https://goo.gle/4p0Pszm GitHub with demo code ‚Üí https://goo.gle/48N83IN Connect with us Remik ‚Üí LinkedIn , X , GitHub Vlad ‚Üí LinkedIn , X , GitHub</description><pubDate>Thu, 22 Jan 2026 11:42:00 +0000</pubDate><guid isPermaLink="false">https://cloud.google.com/blog/topics/developers-practitioners/agent-factory-recap-antigravity-and-nano-banana-pro-with-remik/</guid><source url="https://cloudblog.withgoogle.com/rss">Google Cloud Blog</source><score>9.5</score></item><item><title>The best fitness trackers for 2026</title><link>https://www.engadget.com/wearables/best-fitness-trackers-133053484.html?src=rss</link><description>If you're looking to get fit, sleep better or just keep a closer eye on your health, a fitness wearable is a great place to start. Whether you're into intense workouts or just want to hit your step goal each day, the best fitness trackers available today can offer loads of helpful features, from sleep tracking and resting heart rate monitoring to built-in GPS and stress tracking. Some are even subtle enough to wear 24/7, like smart rings, while others double as stylish smartwatches. There are great options out there for beginners as well as more advanced users, and the variety of features means there‚Äôs something for every lifestyle and budget. In this guide, we‚Äôll walk you through the best fitness trackers you can buy right now, and explain who each one is best suited for. Best fitness trackers for 2026 What do fitness trackers do best? The answer seems simple: Fitness wearables are best at monitoring exercise, be it a 10-minute walk around the block or that half marathon you‚Äôve been diligently training for. Obviously, smartwatches can help you reach your fitness goals too, but there are some areas where fitness bands and smart rings have proven to be the best buy: focus, design, better battery life, durability and price. When I say ‚Äúfocus,‚Äù I‚Äôm alluding to the fact that fitness trackers are made to track activity well; anything else is extra. They often don‚Äôt have the bells and whistles that smartwatches do, which could distract from their advanced health tracking abilities ‚Äî things like all-day resting heart rate monitoring, stress tracking, and even detailed sleep tracker insights. They also tend to have fewer sensors and internal components, which keeps them smaller and lighter. Fitness trackers are also a better option for those who just want a less conspicuous gadget on their wrists all day. Battery life tends to be better on fitness trackers, too. While most smartwatches last one to two days on a single charge, fitness bands offer between five and seven days of battery life ‚Äî and that‚Äôs with all-day and all-night use even with sleep tracking features enabled. Many fitness trackers also slot nicely into your existing ecosystem, syncing seamlessly with your smartphone, other fitness apps and cloud storage to keep all your data in one place. When it comes to price point, there‚Äôs no competition. Most worthwhile smartwatches start at $175 to $200, but you can get a solid smart band starting at $70. That makes them a great entry point for beginners who want to track their progress without committing to a full smartwatch. Yes, more expensive bands and smart rings exist (and we recommend a few here), but you‚Äôll find more options under $150 in the fitness tracker space than in the smartwatch space. When to get a smartwatch instead If you need a bit more from your wearable and don‚Äôt want to be limited to a fitness or activity tracker, a smartwatch may be the best buy for you. There are things like on-watch apps, alerts and even more robust fitness features that smartwatches have and the best fitness trackers don‚Äôt. You can use one to control smart home appliances, set timers and reminders, check weather reports and more. Some smartwatches let you choose which apps you want to receive alerts from, and the options go beyond just call and text notifications. Just make sure your smartwatch is compatible with your Android or iPhone, however, before purchasing, as not all of them work with both operating systems. But the extra fitness features are arguably the most important thing to think about when deciding between a fitness tracker and a smartwatch. The latter devices tend to be larger, giving them more space for things like GPS, barometers, onboard music storage and more. While you can find built-in GPS on select fitness trackers, it‚Äôs not common. If you‚Äôre someone who‚Äôs seriously training ‚Äî say for a race or an endurance challenge ‚Äî a dedicated running watch may be worth considering. These often provide more in-depth cardio analytics, recovery insights, and real-time pace data that go beyond what standard trackers can deliver. Other fitness trackers we've tested Fitbit Inspire 3 The Fitbit Inspire 3 strips out all the luxury features from the Charge 6 and keeps only the essential tracking features. You won‚Äôt get built-in GPS tracking or Fitbit Pay or Spotify control but you do get solid activity tracking, automatic workout detection, smartphone alerts and plenty more. The updated version has a sleeker design and includes a color touch display and connected GPS, the latter of which lets you track pace and distance while you run or bike outside while you have your phone with you. When compared to the Charge 6, the Inspire 3 is more fashionable, too. Its interchangeable bands let you switch up the look and feel of your tracker whenever you want, and it‚Äôs slim enough to blend in with other jewelry you might be wearing. We were also impressed by its multi-day battery life: Fitbit promises up to 10 days on a single charge, and that checked out for us. After four days of round-the-clock use, the Inspire 3 still had 66 percent battery left to go. Fitness tracker FAQs How long do fitness tracker batteries last? The battery life of fitness trackers can vary depending on the model and its features. On average, most fitness trackers last between five to seven days on a single charge. Basic models with limited features could stretch up to 10 days or more. However, more advanced trackers with features like continuous heart rate monitoring, GPS, or always-on displays may need recharging after one to three days. If you're using GPS or streaming music through your fitness tracker, you'll find that this drains the battery faster. By using these features less, or turning them off, you'll extend battery life. This article originally appeared on Engadget at https://www.engadget.com/wearables/best-fitness-trackers-133053484.html?src=rss</description><pubDate>Thu, 22 Jan 2026 10:00:35 +0000</pubDate><guid isPermaLink="false">4ad1be40-0a29-3f30-89c3-c8f1a9034ccf</guid><source url="https://www.engadget.com/rss.xml">Engadget</source><score>8.6</score></item><item><title>AI-Assisted Coding Requires Constraints</title><link>https://dev.to/ben-santora/ai-code-requires-constraints-2jlb</link><description>The rapid adoption of AI-assisted coding is reshaping how software is produced and deployed. In 2026, AI-assisted coding is no longer confined to startups and hobbyists. It is embedded across the software industry, from companies like Google and NVIDIA to financial institutions, cloud providers, and increasingly, government agencies. AI systems are used to generate boilerplate, refactor legacy code, write tests and in general, accelerate development. AI assistance is now simply part of the standard development environment, not a separate workflow. But as security expert David Mytton (founder and CEO of developer security provider Arcjet) and many other have argued, this shift introduces a new category of risk - that risk being driven not necessarily by bad code than by insufficient understanding of what is being shipped. The problem is not that AI-generated code typically fails outright. It often appears correct enough to deploy while quietly embedding assumptions, edge cases, or security weaknesses that have not been examined. AI-assisted coding often ‚Äúworks,‚Äù but its 'correctness' is inferred from surface behavior rather than established through strong constraints or deep review. When applied to security-sensitive or foundational systems, this can have serious consequences. Even Linus Torvalds has said that he feels AI-assisted coding can be appropriate for prototypes, experiments and to help beginning coders get started. Risk escalates when this relaxed approach is applied indiscriminately to production systems, where dangerous failures and security risks may be created - these can be silent and cumulative. These risks are well-known and widely discussed everywhere. Still, AI-assisted coding makes invention cheap and persuasive, and there are no signs that will be abandoned. It seems inevitable that unverifiable designs will slip into production as a matter of course going forward. Programming language choice can significantly help mitigate this risk and act as an effective constraint. Dynamic, permissive languages such as Python, JavaScript, Ruby, and PHP sit at the high-risk end. They allow code to run with minimal upfront validation, pushing most errors to runtime. With AI in the loop, this makes it easy to deploy code that appears correct but fails under specific conditions. JavaScript adds additional risk through implicit coercions, complex asynchronous behavior, and a vast dependency ecosystem. C and C++ present a different but equally serious risk. Although compiled and statically typed, they do not enforce memory safety or prevent undefined behavior. This is no secret and is discussed often. But C is everywhere and must be dealt with - a skilled and seasoned C developer will know to carefully scan code for this type of error. Without the oversight of such a person, AI-generated code may compile cleanly. yet contain latent vulnerabilities such as buffer overflows or use-after-free bugs that can surface long after deployment. Languages like Java, C#, and Go occupy a middle ground, enforcing stronger structure and surfacing more errors early, but they can still allow logic and security flaws to pass through compilation. At the lower-risk end are languages such as Swift and Kotlin, which enforce stronger type systems, null-safety, and safer defaults than older languages. But many would agree that Rust may be the most risk-averse language available. Its notoriously strict compiler enforces invariants around memory safety, lifetimes, and concurrency that cannot be bypassed accidentally, forcing many classes of errors to fail early and visibly. While this does not entirely prevent logic or design mistakes, it substantially reduces silent failure. I have some experience with Rust and its compiler's ability to provide useful and actionable corrections in its error messages. It is indeed more difficult to get bad code through that compiler. But it can be done. As part of my recent work - testing and pushing the limits of large language models, I asked KIMI to do just that - to create a section of code with flaws that would get by the compiler and it was able to do so. I'm not skilled enough to understand if a skilled and diligent Rust developer would have caught such an error - probably. The broader lesson is that AI-assisted coding is not inherently reckless, but it is viable only where constraints are strong. The proper environment must be provided - strong compilers, comprehensive testing, limited permissions, and meaningful review by skilled human developers. In such an environment, AI-assisted coding can be both productive and relatively safe. As I conclude with all of my other testing involving AI, it's crucial to keep humans in the loop. Ben Santora - January 2026</description><pubDate>Thu, 22 Jan 2026 12:12:31 +0000</pubDate><guid isPermaLink="false">https://dev.to/ben-santora/ai-code-requires-constraints-2jlb</guid><source url="https://dev.to/feed">Dev.to</source><score>10.0</score></item><item><title>Zenflow by Zencoder</title><link>https://www.producthunt.com/products/zencoder</link><description>Specification-driven AI development Discussion | Link</description><pubDate>2026-01-14T08:12:12-08:00</pubDate><guid isPermaLink="false">tag:www.producthunt.com,2005:Post/1062939</guid><source url="https://www.producthunt.com/feed">Product Hunt</source><score>9.7</score></item><item><title>What Makes AI Research Replicable? Executable Knowledge Graphs as Scientific Knowledge Representations</title><link>https://arxiv.org/abs/2510.17795</link><description>arXiv:2510.17795v2 Announce Type: replace-cross Abstract: Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a pluggable, paper-centric knowledge base that automatically integrates code snippets and technical insights extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code is available at https://github.com/zjunlp/xKG.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2510.17795v2</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>9.2</score></item><item><title>Protocode: Prototype-Driven Interpretability for Code Generation in LLMs</title><link>https://arxiv.org/abs/2509.25247</link><description>arXiv:2509.25247v2 Announce Type: replace-cross Abstract: Since the introduction of Large Language Models (LLMs), they have been widely adopted for various tasks such as text summarization, question answering, speech-to-text translation, and more. In recent times, the use of LLMs for code generation has gained significant attention, with tools such as Cursor and Windsurf demonstrating the ability to analyze massive code repositories and recommend relevant changes. Big tech companies have also acknowledged the growing reliance on LLMs for code generation within their codebases. Although these advances significantly improve developer productivity, increasing reliance on automated code generation can proportionally increase the risk of suboptimal solutions and insecure code. Our work focuses on automatically sampling In-Context Learning (ICL) demonstrations which can improve model performance and enhance the interpretability of the generated code. Using AST-based analysis on outputs from the MBPP test set, we identify regions of code most influenced by the chosen demonstrations. In our experiments, we show that high-quality ICL demonstrations not only make outputs easier to interpret but also yield a positive performance improvement on the pass@10 metric. Conversely, poorly chosen ICL demonstrations affected the LLM performance on the pass@10 metric negatively compared to the base model. Overall, our approach highlights the importance of efficient sampling strategies for ICL, which can affect the performance of the model on any given task.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2509.25247v2</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.0</score></item><item><title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title><link>https://arxiv.org/abs/2411.17792</link><description>arXiv:2411.17792v4 Announce Type: replace-cross Abstract: The alignment of pre-trained LLMs continues to draw significant attention from both industry and academia, aiming to ensure responses that are helpful, harmless, and honest. However, identifying a point in the model's representation subspace that simultaneously satisfies all these properties remains challenging. H3Fusion addresses this challenge by introducing a mixture-of-experts (MoE)-based fusion mechanism that models alignment as a controllable drift within the subspace, guided by a drift-regularization loss to balance competing alignment dimensions. Furthermore, we formulate the alignment by finding a dual objective of harnessing the distance of generated embeddings and alignment embeddings, and introduce a gating loss by canalizing the activations on the contributing experts. Extensive evaluations of three benchmark datasets show that H3Fusion is more helpful, less harmful, and more honest in three aspects: it outperforms each individually aligned model by 11.37%, and provides stronger robustness compared to the state-of-the-art LLM ensemble approaches by 13.77% and model-merging approaches by 6.18%. Code is available at https://github.com/git-disl/h3fusion.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2411.17792v4</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.2</score></item><item><title>A Finite Expression Method for Solving High-Dimensional Committor Problems</title><link>https://arxiv.org/abs/2306.12268</link><description>arXiv:2306.12268v3 Announce Type: replace-cross Abstract: Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coefficients in the expression template are found via reinforcement learning. The FEX-based committor solver is tested on several high-dimensional benchmark problems. It gives comparable or better results than neural network-based solvers. Most importantly, FEX is capable of correctly identifying the algebraic structure of the solution which allows one to reduce the committor problem to a low-dimensional one and find the committor with any desired accuracy.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2306.12268v3</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.3</score></item><item><title>Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability</title><link>https://arxiv.org/abs/2601.09261</link><description>arXiv:2601.09261v2 Announce Type: replace Abstract: Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs. We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model. Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.09261v2</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.9</score></item><item><title>Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure</title><link>https://arxiv.org/abs/2601.15077</link><description>arXiv:2601.15077v1 Announce Type: cross Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15077v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.5</score></item><item><title>Reflecting in the Reflection: Integrating a Socratic Questioning Framework into Automated AI-Based Question Generation</title><link>https://arxiv.org/abs/2601.14798</link><description>arXiv:2601.14798v1 Announce Type: new Abstract: Designing good reflection questions is pedagogically important but time-consuming and unevenly supported across teachers. This paper introduces a reflection-in-reflection framework for automated generation of reflection questions with large language models (LLMs). Our approach coordinates two role-specialized agents, a Student-Teacher and a Teacher-Educator, that engage in a Socratic multi-turn dialogue to iteratively refine a single question given a teacher-specified topic, key concepts, student level, and optional instructional materials. The Student-Teacher proposes candidate questions with brief rationales, while the Teacher-Educator evaluates them along clarity, depth, relevance, engagement, and conceptual interconnections, responding only with targeted coaching questions or a fixed signal to stop the dialogue. We evaluate the framework in an authentic lower-secondary ICT setting on the topic, using GPT-4o-mini as the backbone model and a stronger GPT- 4-class LLM as an external evaluator in pairwise comparisons of clarity, relevance, depth, and overall quality. First, we study how interaction design and context (dynamic vs.fixed iteration counts; presence or absence of student level and materials) affect question quality. Dynamic stopping combined with contextual information consistently outperforms fixed 5- or 10-step refinement, with very long dialogues prone to drift or over-complication. Second, we show that our two-agent protocol produces questions that are judged substantially more relevant and deeper, and better overall, than a one-shot baseline using the same backbone model.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14798v1</guid><source url="https://arxiv.org/rss/cs.LG">arXiv Machine Learning</source><score>8.7</score></item><item><title>SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction</title><link>https://arxiv.org/abs/2601.11396</link><description>arXiv:2601.11396v2 Announce Type: replace Abstract: As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.11396v2</guid><source url="https://arxiv.org/rss/cs.CV">arXiv Computer Vision</source><score>8.0</score></item><item><title>Radially Distorted Homographies, Revisited</title><link>https://arxiv.org/abs/2508.21190</link><description>arXiv:2508.21190v2 Announce Type: replace Abstract: Homographies are among the most prevalent transformations occurring in geometric computer vision and projective geometry, and homography estimation is consequently a crucial step in a wide assortment of computer vision tasks. When working with real images, which are often afflicted with geometric distortions caused by the camera lens, it may be necessary to determine both the homography and the lens distortion-particularly the radial component, called radial distortion-simultaneously to obtain anything resembling useful estimates. When considering a homography with radial distortion between two images, there are three conceptually distinct configurations for the radial distortion; (i) distortion in only one image, (ii) identical distortion in the two images, and (iii) independent distortion in the two images. While these cases have been addressed separately in the past, the present paper provides a novel and unified approach to solve all three cases. We demonstrate how the proposed approach can be used to construct new fast, stable, and accurate minimal solvers for radially distorted homographies. In all three cases, our proposed solvers are faster than the existing state-of-the-art solvers while maintaining similar accuracy. The solvers are tested on well-established benchmarks including images taken with fisheye cameras. A reference implementation of the proposed solvers is made available as part of HomLib (https://github.com/marcusvaltonen/HomLib).</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2508.21190v2</guid><source url="https://arxiv.org/rss/cs.CV">arXiv Computer Vision</source><score>8.7</score></item><item><title>OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</title><link>https://arxiv.org/abs/2503.10331</link><description>arXiv:2503.10331v3 Announce Type: replace Abstract: Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ, and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2503.10331v3</guid><source url="https://arxiv.org/rss/cs.CV">arXiv Computer Vision</source><score>9.3</score></item><item><title>ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data</title><link>https://arxiv.org/abs/2601.15025</link><description>arXiv:2601.15025v1 Announce Type: cross Abstract: While deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as to improve object interpretation from sensor data. Our approach is based on the incremental construction of a 3D Semantic Scene Graph (3DSSG). We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous Graph Neural Network (GNN) to create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15025v1</guid><source url="https://arxiv.org/rss/cs.CV">arXiv Computer Vision</source><score>8.1</score></item><item><title>DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution</title><link>https://arxiv.org/abs/2601.13761</link><description>arXiv:2601.13761v2 Announce Type: replace-cross Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations. The code is available at https://github.com/RUCBM/DARC.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.13761v2</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>9.5</score></item><item><title>How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains</title><link>https://arxiv.org/abs/2601.08134</link><description>arXiv:2601.08134v2 Announce Type: replace Abstract: The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domains, including clinical, financial, legal, and mathematical reasoning, alongside complex general reasoning benchmarks, with correctness annotations provided for all samples. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.08134v2</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.0</score></item><item><title>Personality Editing for Language Models through Adjusting Self-Referential Queries</title><link>https://arxiv.org/abs/2502.11789</link><description>arXiv:2502.11789v4 Announce Type: replace Abstract: Large Language Models (LLMs) are integral to applications such as conversational agents and content creation, where precise control over a model's personality is essential for maintaining tone, consistency, and user engagement. However, prevailing prompt-based or fine-tuning approaches either lack robustness or demand large-scale training data, making them costly and impractical. In this paper, we present PALETTE (Personality Adjustment by LLM SElf-TargeTed quEries), a novel method for personality editing in LLMs. Our approach introduces adjustment queries, where self-referential statements grounded in psychological constructs are treated analogously to factual knowledge, enabling direct editing of personality-related responses. Unlike fine-tuning, PALETTE requires only 12 editing samples to achieve substantial improvements in personality alignment across personality dimensions. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2502.11789v4</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>8.2</score></item><item><title>AStar: Boosting Multimodal Reasoning with Automated Structured Thinking</title><link>https://arxiv.org/abs/2502.02339</link><description>arXiv:2502.02339v4 Announce Type: replace Abstract: Multimodal large language models excel across diverse domains but struggle with complex visual reasoning tasks. To enhance their reasoning capabilities, current approaches typically rely on explicit search or post-training techniques. However, search-based methods suffer from computational inefficiency due to extensive solution space exploration, while post-training methods demand substantial data, computational resources, and often exhibit training instability. To address these challenges, we propose \textbf{AStar}, a training-free, \textbf{A}utomatic \textbf{S}tructured \textbf{t}hinking paradigm for multimod\textbf{a}l \textbf{r}easoning. Specifically, we introduce novel ``thought cards'', a lightweight library of high-level reasoning patterns abstracted from prior samples. For each test problem, AStar adaptively retrieves the optimal thought cards and seamlessly integrates these external explicit guidelines with the model's internal implicit reasoning capabilities. Compared to previous methods, AStar eliminates computationally expensive explicit search and avoids additional complex post-training processes, enabling a more efficient reasoning approach. Extensive experiments demonstrate that our framework achieves 53.9\% accuracy on MathVerse (surpassing GPT-4o's 50.2\%) and 32.7\% on MathVision (outperforming GPT-4o's 30.4\%). Further analysis reveals the remarkable transferability of our method: thought cards generated from mathematical reasoning can also be applied to other reasoning tasks, even benefiting general visual perception and understanding. AStar serves as a plug-and-play test-time inference method, compatible with other post-training techniques, providing an important complement to existing multimodal reasoning approaches.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2502.02339v4</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>9.6</score></item><item><title>Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education</title><link>https://arxiv.org/abs/2601.14560</link><description>arXiv:2601.14560v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14560v1</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>9.2</score></item><item><title>Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2601.14270</link><description>arXiv:2601.14270v1 Announce Type: new Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14270v1</guid><source url="https://arxiv.org/rss/cs.CL">arXiv Computation &amp; Language</source><score>9.0</score></item><item><title>MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks</title><link>https://arxiv.org/abs/2601.14652</link><description>arXiv:2601.14652v1 Announce Type: cross Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14652v1</guid><source url="https://rss.arxiv.org/rss/cs.CL">arXiv cs.CL</source><score>8.0</score></item><item><title>Say Anything but This: When Tokenizer Betrays Reasoning in LLMs</title><link>https://arxiv.org/abs/2601.14658</link><description>arXiv:2601.14658v1 Announce Type: new Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct "words" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14658v1</guid><source url="https://rss.arxiv.org/rss/cs.CL">arXiv cs.CL</source><score>8.1</score></item><item><title>Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</title><link>https://arxiv.org/abs/2512.10758</link><description>arXiv:2512.10758v3 Announce Type: replace-cross Abstract: The proliferation of generative AI tools has rendered traditional modular assessments in computing and data-centric education increasingly ineffective, creating a disconnect between academic evaluation and authentic skill measurement. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and empirical validation. We make three primary contributions. First, we establish two formal propositions. (1) Assessments composed of interconnected problems, in which outputs serve as inputs to subsequent tasks, are inherently more AI-resilient than modular assessments due to their reliance on multi-step reasoning and sustained context. (2) Semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution templates. These results challenge widely cited recommendations in recent institutional and policy guidance that promote open-ended assessments as inherently more robust to AI assistance. Second, we validate these propositions through empirical analysis of three university data science courses (N = 117). We observe a substantial AI inflation effect: students achieve near-perfect scores on AI-assisted modular homework, while performance drops by approximately 30 percentage points on proctored exams (Cohen d = 1.51). In contrast, interconnected projects remain strongly aligned with modular assessments (r = 0.954, p &lt; 0.001) while maintaining AI resistance, whereas proctored exams show weaker alignment (r = 0.726, p &lt; 0.001). Third, we translate these findings into a practical assessment design procedure that enables educators to construct evaluations that promote deeper engagement, reflect industry practice, and resist trivial AI delegation.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2512.10758v3</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.4</score></item><item><title>From Charts to Code: A Hierarchical Benchmark for Multimodal Models</title><link>https://arxiv.org/abs/2510.17932</link><description>arXiv:2510.17932v2 Announce Type: replace-cross Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2510.17932v2</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.8</score></item><item><title>ThinkRec: Thinking-based recommendation via LLM</title><link>https://arxiv.org/abs/2505.15091</link><description>arXiv:2505.15091v4 Announce Type: replace-cross Abstract: Recent advances in large language models (LLMs) have enabled more semantic-aware recommendations through natural language generation. Existing LLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like manner, relying on superficial features to match similar items based on click history, rather than reasoning through deeper behavioral logic. This often leads to superficial and erroneous recommendations. Motivated by this, we propose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1 to System 2 (rational system). Technically, ThinkRec introduces a thinking activation mechanism that augments item metadata with keyword summarization and injects synthetic reasoning traces, guiding the model to form interpretable reasoning chains that consist of analyzing interaction histories, identifying user preferences, and making decisions based on target items. On top of this, we propose an instance-wise expert fusion mechanism to reduce the reasoning difficulty. By dynamically assigning weights to expert models based on users' latent features, ThinkRec adapts its reasoning path to individual users, thereby enhancing precision and personalization. Extensive experiments on real-world datasets demonstrate that ThinkRec significantly improves the accuracy and interpretability of recommendations. Our implementations are available at https://github.com/Yu-Qi-hang/ThinkRec.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2505.15091v4</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.9</score></item><item><title>Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</title><link>https://arxiv.org/abs/2601.15195</link><description>arXiv:2601.15195v1 Announce Type: cross Abstract: AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15195v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.6</score></item><item><title>Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback</title><link>https://arxiv.org/abs/2601.15188</link><description>arXiv:2601.15188v1 Announce Type: cross Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15188v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.1</score></item><item><title>Vision-Language Models on the Edge for Real-Time Robotic Perception</title><link>https://arxiv.org/abs/2601.14921</link><description>arXiv:2601.14921v1 Announce Type: cross Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14921v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.9</score></item><item><title>Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering</title><link>https://arxiv.org/abs/2601.14470</link><description>arXiv:2601.14470v1 Announce Type: cross Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages. Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14470v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>9.1</score></item><item><title>Agentic AI Meets Edge Computing in Autonomous UAV Swarms</title><link>https://arxiv.org/abs/2601.14437</link><description>arXiv:2601.14437v1 Announce Type: cross Abstract: The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14437v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>9.0</score></item><item><title>How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework</title><link>https://arxiv.org/abs/2601.15153</link><description>arXiv:2601.15153v1 Announce Type: new Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.15153v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>9.1</score></item><item><title>Implementing Knowledge Representation and Reasoning with Object Oriented Design</title><link>https://arxiv.org/abs/2601.14840</link><description>arXiv:2601.14840v1 Announce Type: new Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation &amp; Reasoning (KR&amp;R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&amp;R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14840v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.5</score></item><item><title>"Just in Time" World Modeling Supports Human Planning and Reasoning</title><link>https://arxiv.org/abs/2601.14514</link><description>arXiv:2601.14514v1 Announce Type: new Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.</description><pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate><guid isPermaLink="false">oai:arXiv.org:2601.14514v1</guid><source url="https://rss.arxiv.org/rss/cs.AI">arXiv cs.AI</source><score>8.1</score></item><item><title>Meta's AI lab ships first models internally after six months as CTO says big leaps for everyday users may be over</title><link>https://the-decoder.com/metas-ai-lab-ships-first-models-internally-after-six-months-as-cto-says-big-leaps-for-everyday-users-may-be-over/</link><description>Meta's new AI lab delivers first models after just six months, but CTO Bosworth is tempering expectations: the era of big leaps for everyday users may be over. The article Meta's AI lab ships first models internally after six months as CTO says big leaps for everyday users may be over appeared first on The Decoder .</description><pubDate>Wed, 21 Jan 2026 20:03:29 +0000</pubDate><guid isPermaLink="false">https://the-decoder.com/?p=31260</guid><source url="https://the-decoder.com/feed/">The Decoder</source><score>8.1</score></item><item><title>Train Your Large Model on Multiple GPUs with Tensor Parallelism</title><link>https://machinelearningmastery.com/train-your-large-model-on-multiple-gpus-with-tensor-parallelism/</link><description>This article is divided into five parts; they are: ‚Ä¢ An Example of Tensor Parallelism ‚Ä¢ Setting Up Tensor Parallelism ‚Ä¢ Preparing Model for Tensor Parallelism ‚Ä¢ Train a Model with Tensor Parallelism ‚Ä¢ Combining Tensor Parallelism with FSDP Tensor parallelism originated from the Megatron-LM paper.</description><pubDate>Wed, 31 Dec 2025 21:22:39 +0000</pubDate><guid isPermaLink="false">https://machinelearningmastery.com/?p=22980</guid><source url="https://machinelearningmastery.com/blog/feed/">Machine Learning Mastery</source><score>8.9</score></item><item><title>Decoupling the AI Stack: How to Architect a Production-Grade Local LLM System</title><link>https://dev.to/chnghia/decoupling-the-ai-stack-how-to-architect-a-production-grade-local-llm-system-1a0c</link><description>From "Localhost" to "On-Premise": An open-source blueprint for building a privacy-first, scalable AI infrastructure with vLLM and LiteLLM. We are currently living in the "Golden Age" of Local AI. Tools like Ollama and LM Studio have democratized access to Large Language Models (LLMs), allowing any developer to spin up a 7B parameter model on their laptop in minutes. However, a significant gap remains in the ecosystem. While these tools are fantastic for single-user experimentation , they often encounter bottlenecks when promoted to a shared, enterprise environment . When you try to move from a "Hobbyist" setup to a "Production" on-premise infrastructure for your team, you face different challenges: Concurrency: How do you serve multiple concurrent users without queuing requests indefinitely? Decoupling: How do you swap models (e.g., Llama 3 to Qwen 2.5) without breaking client applications? Governance: How do you manage API keys, log usage, and enforce budget limits? This article explores an architectural approach to solving these problems by decoupling the AI stack . I will also introduce SOLV Stack , an open-source reference implementation I built to demonstrate this architecture. The Architectural Shift: Decoupling Components In traditional web development, we wouldn't connect our frontend directly to our database. We use API Gateways and Backend services. We need to apply the same rigor to AI Infrastructure. A production-grade Local AI system should be composed of three distinct, loosely coupled layers: The Presentation Layer (UI): Where users interact (Chat interface). The Governance Layer (Gateway): Where routing, logging, and auth happen. The Inference Layer (Compute): Where the raw model processing occurs. By separating these concerns, we avoid vendor lock-in and ensure scalability. The Reference Architecture (SOLV) To implement this philosophy practically, I created a Dockerized boilerplate called SOLV Stack . It stands for the four core components selected for their performance and enterprise readiness: S earXNG (Privacy-focused Search) O penWebUI (The Interface) L iteLLM (The Gateway) V LLM (The Inference Engine) Here is how data flows through the system: 1. The Inference Layer: Why vLLM? For local development, tools like Ollama (based on llama.cpp) are excellent. However, for a shared infrastructure, throughput is king. I chose vLLM for this stack because of its PagedAttention technology. In a multi-user scenario, vLLM manages GPU memory much more efficiently than standard loaders, allowing for higher continuous batching. It is designed to be a server first, maximizing the utilization of your expensive GPUs (like the RTX 5090). 2. The Gateway Layer: The Power of LiteLLM This is perhaps the most critical component for an enterprise architecture. LiteLLM acts as a universal proxy. It normalizes all inputs to the OpenAI standard format. This means your client applications (whether it's OpenWebUI, a custom React app, or an IDE plugin like Continue) only need to know how to speak "OpenAI." They don't need to know if the backend is running vLLM, Azure, or Anthropic. This enables a Hybrid Architecture : Routine tasks: Route to local vLLM (Zero cost, 100% privacy). Complex reasoning: Route to GPT-4 (Pay per token). This logic is handled strictly at the config level, not in your application code. 3. The Interface: OpenWebUI Currently, OpenWebUI offers the most comprehensive feature set for teams, including RAG (Retrieval Augmented Generation) pipelines, user role management, and chat history. Because our stack is decoupled, if a better UI comes out next year (e.g., LibreChat), you can swap this layer without touching your backend models. Implementation: The SOLV-Stack Boilerplate I have packaged this entire architecture into a docker-compose setup that supports NVIDIA GPUs on both Linux and Windows (WSL2) ‚Äîa crucial feature for organizations where developers work on Windows machines. Configuration Example The magic happens in the litellm_config.yaml . Here, we map our internal vLLM instance to a user-facing model name: model_list : # The client sees "gpt-4-local" - model_name : gpt-4-local litellm_params : # But we route it to our local Qwen 2.5 instance model : openai/qwen2.5-coder api_base : http://vllm-backend:8000/v1 api_key : EMPTY Real-World Use Case: The Private Coding Assistant One of the most immediate benefits of this stack is enabling AI coding assistants for your team without sending code to the cloud. Deploy SOLV Stack on a local server with an RTX 5090. Developers install the Continue or Cline extension in VS Code. Point the extension to http://your-server:8080/llm/v1 . Result: A Copilot-like experience that runs entirely within your firewall. Conclusion Building a local AI platform is not just about downloading model weights; it's about designing a system that is stable, observable, and adaptable. By moving from a monolithic "localhost" tool to a decoupled architecture using vLLM and LiteLLM, you gain control over your data and your infrastructure. If you want to try this architecture yourself, I've open-sourced the setup here. It includes scripts for model downloading, Nginx configuration, and RAG pipelines setup. Repo: github.com/chnghia/solv-stack I'd love to hear how you are architecting your local AI stack. Are you using a Gateway pattern? Let me know in the comments!</description><pubDate>Thu, 22 Jan 2026 03:44:17 +0000</pubDate><guid isPermaLink="false">https://dev.to/chnghia/decoupling-the-ai-stack-how-to-architect-a-production-grade-local-llm-system-1a0c</guid><source url="https://dev.to/feed">Dev.to</source><score>8.1</score></item></channel></rss>
